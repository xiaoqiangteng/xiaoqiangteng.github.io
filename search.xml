<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Android开发之使用网络时间</title>
      <link href="/2018/11/23/Android%E5%BC%80%E5%8F%91%E4%B9%8B%E4%BD%BF%E7%94%A8%E7%BD%91%E7%BB%9C%E6%97%B6%E9%97%B4/"/>
      <url>/2018/11/23/Android%E5%BC%80%E5%8F%91%E4%B9%8B%E4%BD%BF%E7%94%A8%E7%BD%91%E7%BB%9C%E6%97%B6%E9%97%B4/</url>
      
        <content type="html"><![CDATA[<h1 id="使用truetime-android第三方库实现"><a href="#使用truetime-android第三方库实现" class="headerlink" title="使用truetime-android第三方库实现"></a>使用truetime-android第三方库实现</h1><p>github地址：<a href="https://github.com/instacart/truetime-android" target="_blank" rel="noopener">https://github.com/instacart/truetime-android</a></p><h2 id="配置教程"><a href="#配置教程" class="headerlink" title="配置教程"></a>配置教程</h2><p>详见：github的<a href="https://github.com/instacart/truetime-android/wiki/How-to-use-this-library" target="_blank" rel="noopener">wiki</a>，也可阅读本文实例教程。</p><p>Add this to your application’s build.gradle file:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">repositories &#123;</span><br><span class="line">    maven &#123;</span><br><span class="line">        url <span class="string">"https://jitpack.io"</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">dependencies &#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    implementation <span class="string">'com.github.instacart.truetime-android:library-extension-rx:&lt;release-version&gt;'</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// or if you want the vanilla version of Truetime:</span></span><br><span class="line">    implementation <span class="string">'com.github.instacart.truetime-android:library:&lt;release-version&gt;'</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>例子：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">allprojects &#123;</span><br><span class="line">    repositories &#123;</span><br><span class="line">        google()</span><br><span class="line">        jcenter()</span><br><span class="line">        maven &#123;</span><br><span class="line">            url <span class="string">"https://jitpack.io"</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">dependencies &#123;</span><br><span class="line">    <span class="function">implementation <span class="title">fileTree</span><span class="params">(dir: <span class="string">'libs'</span>, include: [<span class="string">'*.jar'</span>])</span></span></span><br><span class="line"><span class="function">    implementation 'com.android.support:appcompat-v7:28.0.0-rc02'</span></span><br><span class="line"><span class="function">    implementation 'com.android.support.constraint:constraint-layout:1.1.2'</span></span><br><span class="line"><span class="function">    implementation 'com.github.instacart.truetime-android:library:3.3'</span></span><br><span class="line"><span class="function">&#125;</span></span><br></pre></td></tr></table></figure><p>Importing ‘com.github.instacart.truetime-android:library:<release-version>‘ should be sufficient for this.</release-version></p><p>Then you must initialize it in onCreate() in your class that extendsandroid.app.Application.</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">TrueTime.build().initialize();</span><br></pre></td></tr></table></figure><p>例子：</p><p>APP.java</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">App</span> <span class="keyword">extends</span> <span class="title">Application</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">onCreate</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>.onCreate();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> Thread(<span class="keyword">new</span> Runnable() &#123;</span><br><span class="line">            <span class="meta">@Override</span></span><br><span class="line">            <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">run</span><span class="params">()</span> </span>&#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    TrueTime.build().withNtpHost(<span class="string">"time.ustc.edu.cn"</span>).initialize();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;).start();</span><br><span class="line"></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>将APP.java添加到AndroidManifest.xml</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;application</span><br><span class="line">android:name=<span class="string">".App"</span></span><br><span class="line">&lt;/application&gt;</span><br></pre></td></tr></table></figure><p>使用Date noReallyThisIsTheTrueDateAndTime = TrueTime.now();来获取网络时间。</p>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
          <category> Android </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
            <tag> Android </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2D-3D-S数据集介绍</title>
      <link href="/2018/09/14/2D-3D-S%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D/"/>
      <url>/2018/09/14/2D-3D-S%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>数据集详情地址：<a href="http://3Dsemantics.stanford.edu/" target="_blank" rel="noopener">http://3Dsemantics.stanford.edu/</a></p><p>该数据集构建的主要目标是为场景理解、深度估计、法线估计、实体检测、分割和场景重建提供研究数据集，推动该领域的发展。</p><h1 id="相关数据集介绍"><a href="#相关数据集介绍" class="headerlink" title="相关数据集介绍"></a>相关数据集介绍</h1><p>现有的基于RGB-D数据集包括NYU Depth v2、SUN RGBD和SceneNN。然而，这些数据集对于处理特定的任务和2.5D的场景有限。</p><h1 id="采集和处理"><a href="#采集和处理" class="headerlink" title="采集和处理"></a>采集和处理</h1><h2 id="3D模式"><a href="#3D模式" class="headerlink" title="3D模式"></a>3D模式</h2><p>3D模式主要包含点云和mesh。文章利用Matterport Camera得到3D textured Mesh model，然后利用采样方法得到3D点运数据。此外，文章将点运数据与颜色进行对应来生成colored点云数据。</p><p>语义标记：文章对点云数据中的每个Mesh和Voxel进行语义信息的标记。语义主要包括ceiling, floor, wall, beam, column, window, door, table, chair, sofa, bookcase, board and clutter。此外，语义信息被投影到RGB图像中。进一步地，文章划分数据为如下室内场景：office, conference room, hallway, auditorium, open space, lobby, lounge, pantry, copy room, storage and WC。每个对象被存储在文件名为“class instanceNum roomType roomNum areaNum”中。</p><blockquote><p>The dataset contains colored point clouds and textured meshes for each scanned area. 3D semantic annotations for objects and scenes are offered for both modalities, with point-level and face-level labels correspondingly. The annotations were initially performed on the point cloud and then projected onto the closest surface on the 3D mesh model. Faces in the 3D mesh that account for no projected points belong to non-annotated parts of the dataset and are labeled with a default null value. We also provide the tightest axis-aligned bounding box of each object instance and further voxelize it into a 6x6x6 grid with binary occupancy and point correspondence.</p></blockquote><p>该数据集包含每个扫描区域的带有颜色信息的点云和纹理mesh，分别提供点粒度和面粒度的实体和场景的语义信息。最初，语义信息被提取自点云数据，之后映射到最相邻的三维mesh上。不能够被点云映射的面的语义信息置为空值。该数据集也提供了每隔实体样例的tightest axis-aligned bounding box和使用6x6x6二进制的网格来进行体素化表达。</p><h3 id="技术细节"><a href="#技术细节" class="headerlink" title="技术细节"></a>技术细节</h3><p>该数据集包含颜色点云和3D mesh。</p><h4 id="颜色点云数据"><a href="#颜色点云数据" class="headerlink" title="颜色点云数据"></a>颜色点云数据</h4><blockquote><p>The raw colored 3D point clouds along with both object and scene instance-level annotations per point, (tightest) axis-aligned bounding boxes and voxels with binary occupancy and point correpsondence are stored in the Area_#_PointCloud.mat file. The variables are stored in the form of nested structs</p></blockquote><p>具有实体和场景粒度的语义信息、axis-aligned bounding boex和具有点对应的二进制占据体素的三维点云数据被存储在“Area_#_PointCloud.mat”文件中。变量被存储为嵌套结构体：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">- Area: --&gt; name: the area name as: Area_#</span><br><span class="line">--&gt; Disjoint-Space: <span class="class"><span class="keyword">struct</span> <span class="title">with</span> <span class="title">information</span> <span class="title">on</span> <span class="title">the</span> <span class="title">individual</span> <span class="title">spaces</span> <span class="title">in</span> <span class="title">the</span> <span class="title">building</span>.</span></span><br></pre></td></tr></table></figure><p>使用matlab读取后结构体显示如下：</p><p><img src alt></p><p>其中，Disjoint_Space结构体为</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- Disjoint_Space:--&gt; name: the name of that space, <span class="function">with per area global <span class="title">index</span> <span class="params">(e.g. conferenceRoom_1, offie_13, etc.)</span></span></span><br><span class="line">--&gt;AlignmentAngle: rotation angle around Z axis, to align spaces based on the CVPR2016 paper *3D Semantic Parsing of Large-Scale Indoor Spaces*. </span><br><span class="line">--&gt; color: a unique RGB color value [<span class="number">0</span>,<span class="number">1</span>] <span class="keyword">for</span> that room, mainly <span class="keyword">for</span> visualization purposes</span><br><span class="line">--&gt; object: a <span class="class"><span class="keyword">struct</span> <span class="title">that</span> <span class="title">contains</span> <span class="title">all</span> <span class="title">objects</span> <span class="title">in</span> <span class="title">that</span> <span class="title">space</span>.</span></span><br></pre></td></tr></table></figure><p>使用matlab读取后结构体显示如下：</p><p><img src alt></p><p>object结构体为：</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">- object:--&gt; name: the name of the object, wiith per space indexing* (e.g. chair<span class="number">-1</span>, wall_3, clutter_13, etc.)</span><br><span class="line">--&gt; points: the X,Y,Z coordinates of the <span class="number">3</span>D points that comprise <span class="keyword">this</span> object</span><br><span class="line">--&gt; RGB_color: the raw RGB color value [<span class="number">0</span>,<span class="number">255</span>] associated with each point.</span><br><span class="line">--&gt; global_name: the name of the object, with per area global index**</span><br><span class="line">--&gt; Bbox: [Xmin Xmax Ymin Ymax Zmin Zmax] of the object's boudning box</span><br><span class="line">--&gt; Voxels: [Xmin Xmax Ymin Ymax Zmin Zmax] <span class="keyword">for</span> each voxel in a <span class="number">6</span>x6x6 grid</span><br><span class="line">--&gt; Voxel_Occupancy: <span class="function">binary occupancy per <span class="title">voxel</span> <span class="params">(<span class="number">0</span>: empty, <span class="number">1</span>: contains points)</span></span></span><br><span class="line">--&gt; Points_per_Voxel: the object points that correspond to each voxel (in XYZ coordinates)</span><br></pre></td></tr></table></figure><p><img src alt></p><h4 id="3D-Semantic-mesh"><a href="#3D-Semantic-mesh" class="headerlink" title="3D Semantic mesh"></a>3D Semantic mesh</h4><blockquote><p>The 3D semantic mesh is labeled with instance-level per-face annotations. The mesh is stored in semantic.obj and semantic.mtl where face labels are stored as face’s material’s name. In Blender, for example, the material label can be retrieved with:</p></blockquote><p>三维语义mesh被使用具有实例粒度的每个面元的注释。mesh被存储在”semantic.obj”和”semantic.mtl”文件中。其中，面元标签被存储在面元的名字。在Blender中，标签可悲解析如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mesh_material_idx = mesh.data.polygons[ face_idx ].material_index</span><br><span class="line">material = mesh.data.materials[ mesh_material_idx ]</span><br><span class="line">label = material.name   # final label!</span><br></pre></td></tr></table></figure><p>标签格式如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class_instanceNum_roomType_roomNum_areaNum</span><br></pre></td></tr></table></figure><p>When using the mesh’s color, the material’s color should be remapped for the task at hand as the default color is designed for visualizations. One way to encode the label in the color is to set</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">material.diffuse_color = get_color( labels.index( material.name ) )</span><br></pre></td></tr></table></figure><p>by using labels from /assets/semantic_labels.json and get_color( color ) from /assets/utils.py.</p><h2 id="2D模态"><a href="#2D模态" class="headerlink" title="2D模态"></a>2D模态</h2><p>RGB图像：采集朝向：yaw角约[-180, 180], pitch角约为以0为均值15度为方差的高斯分布，roll角一直是0度。FOV角为以75度为均值和-30度为标准差的半高斯分布。为了过滤掉白墙等图像，文章基于语义信息商来过滤掉近70%的图像。</p><p>语义信息商的计算方法如下：首先，对于每一幅图像对13类别的语义信息进行像素统计。再利用像素分布计算香农商。最后，将低于15%的图像过滤掉。</p><p>此外，我们保留60%的图像。剩下的25%的图像我们通过半高斯分布采样方法来筛选。如此，文章保证了图像的多样性。</p><blockquote><p>The dataset contains densely sampled RGB images per scan location. These images were sampled from equirectangular images that were generated per scan location and modality using the raw data captured by the scanner. All images in the dataset are stored in full high-definition at 1080x1080 resolution. For more details on the random sampling of RGB images read section 4.2 in the paper. We provide the camera metadata for each generated image.</p></blockquote><blockquote><p>We also provide depth images that were computed on the 3D mesh instead of directly on the 3D mesh, as well as surface normal images. 2D semantic annotations are computed for each image by projecting the 3D mesh labels on the image plane. Due to certain geo- metric artifacts present at the mesh model mainly because of the level of detail in the reconstruction, the 2D annotations occasionally present small local misalignment to the underlying pixels, especially for points that have a short distance to the camera. This issue can be easily addressed by fusing image content with the projected annotations using graphical models. The dataset also includes 3D coordinate encoded images where each pixel encodes the X, Y, Z location of the point in the world coordinate system. Last, an equirectangular projection is also provided per scan location and modality.</p></blockquote><p>该数据集包含了每个采集区域的稠密的RGB图像。作者使用扫描仪输出的全景图像来生成这些图像。在数据集里的所有的图像的分辨率均为1080x1080。细节请参考论文。</p><p>此外，数据集也包含深度图像。作者使用三维mesh的语义信息，通过映射到图像平面来得到图像中的语义信息。数据集中每幅图像的三维坐标X,Y,Z均在世界坐标系下。</p><h3 id="技术细节-1"><a href="#技术细节-1" class="headerlink" title="技术细节"></a>技术细节</h3><h4 id="pose"><a href="#pose" class="headerlink" title="pose"></a>pose</h4><blockquote><p>The pose files contain camera metadata for each image and are given in the /pose subdirectories. They have filenames which are globally unique due to the fact that camera uuids are not shared between areas. They are stored in json files, and contain</p></blockquote><p>pose文件包含每一幅图像的camera元数据，被给定在“/pose”子文件夹下。他们有一个全局的文件名，被存储在json文件中。</p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="attr">"camera_k_matrix"</span>:  # The <span class="number">3</span>x3 camera K matrix. Stored as a list-of-lists, </span><br><span class="line">  "field_of_view_rads": #  The Camera's field of view, in radians, </span><br><span class="line">  "camera_original_rotation": #  The camera's initial XYZ-Euler rotation in the .obj, </span><br><span class="line">  "rotation_from_original_to_point": </span><br><span class="line">  #  Apply this to the original rotation in order to orient the camera for the corresponding picture, </span><br><span class="line">  "point_uuid": #  alias for camera_uuid, </span><br><span class="line">  "camera_location": #  XYZ location of the camera, </span><br><span class="line">  "frame_num": #  The frame_num in the filename, </span><br><span class="line">  "camera_rt_matrix": #  The 4x3 camera RT matrix, stored as a list-of-lists, </span><br><span class="line">  "final_camera_rotation": #  The camera Euler in the corresponding picture, </span><br><span class="line">  "camera_uuid": #  The globally unique identifier for the camera location, </span><br><span class="line">  "room": #  The room that this camera is in. Stored as roomType_roomNum_areaNum </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="RGB"><a href="#RGB" class="headerlink" title="RGB"></a>RGB</h4><blockquote><p>RGB images are in the /rgb folder and contain synthesized but accurate RGB images of the scene.</p></blockquote><p>RGB图像被存储在“/rgb”文件夹中。</p><h4 id="Depth"><a href="#Depth" class="headerlink" title="Depth"></a>Depth</h4><blockquote><p>Depth images are stored as 16-bit PNGs and have a maximum depth of 128m and a sensitivity of 1/512m. Missing values are encoded with the value 2^16 - 1. Note that while depth is defined relative to the plane of the camera in the data (z-depth), it is defined as the distance from the point-center of the camera in the panoramics.</p></blockquote><h4 id="Global-XYZ"><a href="#Global-XYZ" class="headerlink" title="Global XYZ"></a>Global XYZ</h4><blockquote><p>Global XYZ images contain the ground-truth location of each pixel in the mesh. They are stored as 16-bit 3-channel OpenEXR files and a convenience readin function is provided in /assets/utils.py. These can be used for generating point correspondences, e.g. for optical flow. Missing values are encoded as #000000.</p></blockquote><h4 id="Normal"><a href="#Normal" class="headerlink" title="Normal"></a>Normal</h4><blockquote><p>Normals are 127.5-centered per-channel surface normal images. For panoramic images, these normals are relative to the global corodinate system. Since the global coordinate system is impossible to determine from a sampled image, the normal images in /data have their normals defined relative to the direction the camera is facing. The normals axis-color convention is the same one used by NYU RGB-D. Areas where the mesh is missing have pixel color #808080.</p></blockquote><h4 id="Semantic"><a href="#Semantic" class="headerlink" title="Semantic"></a>Semantic</h4><blockquote><p>Semantic images come in two variants, semantic and semantic_pretty. They both include information from the point cloud annotations, but only the semantic version should be used for learning. The labels can be found in assets/semantic_labels.json, and images can be parsed using some of the convenience functions in utils.py. Specifically: The semantic images are encoded as 3-channel 8-bit PNGs which are interpreted as 24-bit base-256 integers which are an index into the labels array in semantic_labels.json.</p></blockquote><p>Let’s say that you’ve loaded the image into memory and it’s stored as a numpy array called img and want the label for the pixel at (1500, 2000) which is the leftmost sofa chair in this image. utils.py provides get_index, load_labels and parse_labels for extracting the label information. Here is what your code might look like:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.misc <span class="keyword">import</span> imread</span><br><span class="line"><span class="keyword">from</span> assets.utils <span class="keyword">import</span> *  <span class="comment"># Assets should be downloaded from this repo</span></span><br><span class="line">labels = load_labels( <span class="string">'/path/to/assets/semantic_labels.json'</span> )</span><br><span class="line"></span><br><span class="line">img = imread(  <span class="string">'/path/to/image.png'</span> )</span><br><span class="line">pix = img[ <span class="number">1500</span>,<span class="number">2000</span> ]</span><br><span class="line">instance_label = labels[ get_index( pix ) ]</span><br><span class="line">instance_label_as_dict = parse_label( instance_label )</span><br><span class="line"><span class="keyword">print</span> instance_label_as_dict</span><br></pre></td></tr></table></figure><p>Gives {‘instance_num’: 5, ‘instance_class’: u’sofa’, ‘room_num’: 3, ‘room_type’: u’office’, ‘area_num’: 3} Here we can see that this is the 5th instance of class ‘sofa’ in area 3.</p><p>Finally, note that pixels where the data is missing are encoded with the color #0D0D0D which is larger than the len( labels ).</p>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
          <category> 数据集 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
            <tag> 数据集 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法-元素最近邻快速查找</title>
      <link href="/2018/09/11/%E7%AE%97%E6%B3%95-%E5%85%83%E7%B4%A0%E6%9C%80%E8%BF%91%E9%82%BB%E5%BF%AB%E9%80%9F%E6%9F%A5%E6%89%BE/"/>
      <url>/2018/09/11/%E7%AE%97%E6%B3%95-%E5%85%83%E7%B4%A0%E6%9C%80%E8%BF%91%E9%82%BB%E5%BF%AB%E9%80%9F%E6%9F%A5%E6%89%BE/</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><h1 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h1><p>需求：给定一个元素，在指定集合中输出n个最近邻的元素。如果集合中没有重复元素，那么只有一个元素输出；如果集合中由重复元素，那么可能存在多个最近邻元素输出。</p><h1 id="求解"><a href="#求解" class="headerlink" title="求解"></a>求解</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ReadCsvData</span><span class="params">(file_name)</span>:</span></span><br><span class="line">    time = []</span><br><span class="line">    loc = []</span><br><span class="line">    <span class="keyword">with</span> open(file_name,<span class="string">'r'</span>) <span class="keyword">as</span> csvfile:</span><br><span class="line">        reader = csv.reader(csvfile)</span><br><span class="line">        rows= [row <span class="keyword">for</span> row <span class="keyword">in</span> reader]</span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> rows:</span><br><span class="line">            data = np.array(data)</span><br><span class="line">            time.append(float(data[<span class="number">0</span>]))</span><br><span class="line">            loc.append([float(data[<span class="number">1</span>]), float(data[<span class="number">2</span>]), float(data[<span class="number">3</span>])])</span><br><span class="line">    time = np.array(time)</span><br><span class="line">    loc = np.array(loc)</span><br><span class="line">    <span class="keyword">return</span> time, loc</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">searchInsert</span><span class="params">(nums, target)</span>:</span></span><br><span class="line">    start = <span class="number">0</span></span><br><span class="line">    end = len(nums) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> start &lt;= end:</span><br><span class="line">        mid = (start + end) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid] == target:</span><br><span class="line">            <span class="keyword">return</span> mid</span><br><span class="line">        <span class="keyword">elif</span> nums[mid] &lt; target:</span><br><span class="line">            start = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            end = mid - <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> end + <span class="number">1</span></span><br><span class="line"> </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rIndex</span><span class="params">(nums, target)</span>:</span></span><br><span class="line">    n = len(nums)</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">0</span>: <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">    mid = searchInsert(nums, target)</span><br><span class="line">    rlist = []  <span class="comment"># 保持索引</span></span><br><span class="line">    i, j = <span class="number">-1</span>, n</span><br><span class="line">    left, rigth = <span class="number">0</span>, <span class="number">0</span>  <span class="comment"># 左右扩展的标志</span></span><br><span class="line">    mxg = float(<span class="string">'-inf'</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="number">0</span> &lt; mid &lt; n:  <span class="comment"># 如果找到了</span></span><br><span class="line">        i, j = mid<span class="number">-1</span>, mid</span><br><span class="line">        mxg = min(abs(nums[i] - target), abs(nums[j] - target))</span><br><span class="line">        left, rigth = <span class="number">1</span>, <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> mid == <span class="number">0</span>:  <span class="comment"># 小于最左边的数字</span></span><br><span class="line">        j = mid</span><br><span class="line">        mxg = abs(nums[j] - target)</span><br><span class="line">        left, rigth = <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">    <span class="keyword">elif</span> mid == n:  <span class="comment"># 大于最右边的数字</span></span><br><span class="line">        i = mid<span class="number">-1</span></span><br><span class="line">        mxg = abs(nums[i] - target)</span><br><span class="line">        left, rigth = <span class="number">1</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> left == <span class="number">1</span> <span class="keyword">or</span> rigth == <span class="number">1</span>:  <span class="comment"># 两边查找</span></span><br><span class="line">        <span class="keyword">if</span> i == <span class="number">-1</span>: left = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> j == n: rigth = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> left == <span class="number">1</span> <span class="keyword">and</span> i &gt;= <span class="number">0</span>:</span><br><span class="line">            le = abs(nums[i] - target)</span><br><span class="line">            <span class="keyword">if</span> le == mxg:</span><br><span class="line">                rlist = [i] + rlist</span><br><span class="line">                i -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                left = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> rigth == <span class="number">1</span> <span class="keyword">and</span> j &lt; len(nums):</span><br><span class="line">            ri = abs(nums[j] - target)</span><br><span class="line">            <span class="keyword">if</span> mxg == ri:</span><br><span class="line">                rlist = rlist + [j]</span><br><span class="line">                j += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                rigth = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> rlist</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment">################# File path #######################</span></span><br><span class="line">    gt_path = <span class="string">"../data/ADVIO/advio-02/ground-truth/pose.csv"</span></span><br><span class="line">    ARKit_path = <span class="string">"../data/ADVIO/advio-02/iphone/arkit.csv"</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">################# Read data #######################</span></span><br><span class="line">    time_gt = []</span><br><span class="line">    loc_gt = []</span><br><span class="line">    time_ARKit = []</span><br><span class="line">    loc_ARKit = []</span><br><span class="line">    errors = []</span><br><span class="line">    time_gt, loc_gt = ReadCsvData(gt_path)</span><br><span class="line">    time_ARKit, loc_ARKit = ReadCsvData(ARKit_path)</span><br><span class="line">    <span class="keyword">print</span> len(time_gt), len(time_ARKit)</span><br><span class="line">    <span class="keyword">if</span> len(time_gt) &lt; len(time_ARKit):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(time_gt)):</span><br><span class="line">            index = rIndex(time_ARKit, time_gt[i])[<span class="number">0</span>]</span><br><span class="line">            error = math.sqrt((loc_gt[i][<span class="number">0</span>] - loc_ARKit[index][<span class="number">0</span>])**<span class="number">2</span> + (loc_gt[i][<span class="number">1</span>] - loc_ARKit[index][<span class="number">1</span>])**<span class="number">2</span> + (loc_gt[i][<span class="number">2</span>] - loc_ARKit[index][<span class="number">2</span>])**<span class="number">2</span>)</span><br><span class="line">            errors.append(error)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(time_ARKit)):</span><br><span class="line">            index = rIndex(time_gt, time_ARKit[i])[<span class="number">0</span>]</span><br><span class="line">            error = math.sqrt((loc_gt[index][<span class="number">0</span>] - loc_ARKit[i][<span class="number">0</span>])**<span class="number">2</span> + (loc_gt[index][<span class="number">1</span>] - loc_ARKit[i][<span class="number">1</span>])**<span class="number">2</span> + (loc_gt[index][<span class="number">2</span>] - loc_ARKit[i][<span class="number">2</span>])**<span class="number">2</span>)</span><br><span class="line">            errors.append(error)</span><br><span class="line">    errors = np.array(errors)</span><br><span class="line">    <span class="keyword">print</span> errors</span><br><span class="line">    plt.figure(<span class="number">1</span>)</span><br><span class="line">    plt.plot(loc_gt[:, <span class="number">0</span>], loc_gt[:, <span class="number">2</span>], c = <span class="string">'r'</span>)</span><br><span class="line">    plt.plot(loc_ARKit[:, <span class="number">0</span>], loc_ARKit[:, <span class="number">2</span>], c = <span class="string">'b'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">"X (m)"</span>)</span><br><span class="line">    plt.ylabel(<span class="string">"Y (m)"</span>)</span><br><span class="line">    plt.legend([<span class="string">"Groud-truth"</span>, <span class="string">"ARKit"</span>])</span><br><span class="line">    plt.figure(<span class="number">2</span>)</span><br><span class="line">    plt.plot(errors, c = <span class="string">'b'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Index'</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'Error (m)'</span>)</span><br><span class="line">    plt.show()</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>apolloScape竞赛</title>
      <link href="/2018/09/09/apolloScape%E7%AB%9E%E8%B5%9B/"/>
      <url>/2018/09/09/apolloScape%E7%AB%9E%E8%B5%9B/</url>
      
        <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><h1 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h1><p>百度ApolloScape重磅发布了自动驾驶开放数据集。自动驾驶开发测试中，海量、高质的真实数据是必不可缺的“原料”。但是，少有团队有能力开发并维持一个适用的自动驾驶平台，定期校准并收集新数据。</p><p>据介绍，Apollo开放平台此次发布的ApolloScape不仅开放了比Cityscapes等同类数据集大10倍以上的数据量，包括感知、仿真场景、路网数据等数十万帧逐像素语义分割标注的高分辨率图像数据，进一步涵盖更复杂的环境、天气和交通状况等。从数据难度上来看，ApolloScape数据集涵盖了更复杂的道路状况（例如，单张图像中多达162辆交通工具或80名行人），同时开放数据集采用了逐像素语义分割标注的方式，是目前环境最复杂、标注最精准、数据量最大的自动驾驶数据集。</p><p>Apollo开放平台还将与加州大学伯克利分校在CVPR 2018（IEEE国际计算机视觉与模式识别会议）期间联合举办自动驾驶研讨会（Workshop on Autonomous Driving），并将基于ApolloScape的大规模数据集定义了多项任务挑战，为全球自动驾驶开发者和研究人员提供共同探索前沿领域技术突破及应用创新的平台。</p><h1 id="参考一：PoseNet-implementation-for-self-driving-car-localization-using-Pytorch-on-Apolloscape-dataset"><a href="#参考一：PoseNet-implementation-for-self-driving-car-localization-using-Pytorch-on-Apolloscape-dataset" class="headerlink" title="参考一：PoseNet implementation for self-driving car localization using Pytorch on Apolloscape dataset"></a>参考一：<a href="https://capsulesbot.com/blog/2018/08/24/apolloscape-posenet-pytorch.html" target="_blank" rel="noopener">PoseNet implementation for self-driving car localization using Pytorch on Apolloscape dataset</a></h1><p>This article covers the very beginning of the journey and includes the reading and visualization of the Apolloscape dataset for localization task. Implement PoseNet [2] architecture for monocular image pose prediction and visualize results. I use Python and Pytorch for the task.</p><p>NOTE: If you want to jump straight to the code here is the GitHub repo. It’s is still an ongoing work where I intend to implement Vidloc [7], Pose Graph Optimization [3,8] and Structure from Motion [9] pipelines for Apolloscape Dataset in the context of the localization task.</p><h2 id="Apolloscape-Pytorch-Dataset"><a href="#Apolloscape-Pytorch-Dataset" class="headerlink" title="Apolloscape Pytorch Dataset"></a>Apolloscape Pytorch Dataset</h2><p>For Pytorch I need to have a Dataset object that prepares and feeds the data to the loader and then to the model. I want to have a robust dataset class that can:</p><ul><li>support stereo and mono images</li><li>support train/validation splits that came along with data or generate a new one</li><li>support pose normalization</li><li>support different pose representations (needed mainly for visualization and experiments with loss functions)</li><li>support filtering by record id</li><li>support general Apolloscape folder structure layout</li></ul><p>I am not putting here the full listing of the Apolloscape dataset and concentrate solely on how to use it and what data we can get from it. For the full source code, please refer to the Github file datasets/apolloscape.py.</p><p>Here how to create a dataset:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> datasets.apolloscape <span class="keyword">import</span> Apolloscape</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># Path to unpacked data folders</span></span><br><span class="line">APOLLO_PATH = <span class="string">"./data/apolloscape"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Resize transform that is applied on every image read</span></span><br><span class="line">transform = transforms.Compose([transforms.Resize(<span class="number">250</span>)])</span><br><span class="line"></span><br><span class="line">apollo_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road=<span class="string">"zpark-sample"</span>,</span><br><span class="line">                             transform=transform, train=<span class="keyword">True</span>, pose_format=<span class="string">'quat'</span>,</span><br><span class="line">                             stereo=<span class="keyword">True</span>)</span><br><span class="line">print(apollo_dataset)</span><br></pre></td></tr></table></figure><p>output:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Dataset: Apolloscape</span><br><span class="line">    Road: zpark-sample</span><br><span class="line">    Record: <span class="keyword">None</span></span><br><span class="line">    Train: <span class="keyword">None</span></span><br><span class="line">    Normalize Poses: <span class="keyword">False</span></span><br><span class="line">    Stereo: <span class="keyword">True</span></span><br><span class="line">    Length: <span class="number">1499</span> of <span class="number">1499</span></span><br><span class="line">    Cameras: [<span class="string">'Camera_2'</span>, <span class="string">'Camera_1'</span>]</span><br><span class="line">    Records: [<span class="string">'Record001'</span>, <span class="string">'Record002'</span>, <span class="string">'Record003'</span>, <span class="string">'Record004'</span>, <span class="string">'Record006'</span>, <span class="string">'Record007'</span>, <span class="string">'Record008'</span>, <span class="string">'Record009'</span>, <span class="string">'Record010'</span>, <span class="string">'Record011'</span>, <span class="string">'Record012'</span>, <span class="string">'Record013'</span>, <span class="string">'Record014'</span>]</span><br></pre></td></tr></table></figure><p>POLLO_PATH is a folder with unpacked Apolloscape datasets, e.g. \$APOLLO_PATH/road02_seg or \$APOLLO_PATH/zpark. Download data from Apolloscape page and unpack iot. Let’s assume that we’ve also created a symlink ./data/apolloscape that points to $APOLLO_PATH folder.</p><p>We can view the list of available records with a number of data samples in each:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Show records with numbers of data points</span></span><br><span class="line">recs_num = apollo_dataset.get_records_counts()</span><br><span class="line">recs_num = sorted(recs_num.items(), key=<span class="keyword">lambda</span> kv: kv[<span class="number">1</span>], reverse=<span class="keyword">True</span>)</span><br><span class="line">print(<span class="string">"Records:"</span>)</span><br><span class="line">print(<span class="string">"\n"</span>.join([<span class="string">"\t&#123;&#125; - &#123;&#125;"</span>.format(r[<span class="number">0</span>], r[<span class="number">1</span>]) <span class="keyword">for</span> r <span class="keyword">in</span> recs_num ]))</span><br></pre></td></tr></table></figure><p>output:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">Records:</span><br><span class="line">Record008 - <span class="number">122</span></span><br><span class="line">Record007 - <span class="number">121</span></span><br><span class="line">Record006 - <span class="number">121</span></span><br><span class="line">Record012 - <span class="number">121</span></span><br><span class="line">Record001 - <span class="number">121</span></span><br><span class="line">Record009 - <span class="number">121</span></span><br><span class="line">Record010 - <span class="number">121</span></span><br><span class="line">Record003 - <span class="number">121</span></span><br><span class="line">Record013 - <span class="number">120</span></span><br><span class="line">Record004 - <span class="number">120</span></span><br><span class="line">Record002 - <span class="number">120</span></span><br><span class="line">Record011 - <span class="number">120</span></span><br><span class="line">Record014 - <span class="number">50</span></span><br></pre></td></tr></table></figure><p>We can draw a route for one record with a sampled camera image:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> utils.common <span class="keyword">import</span> draw_record</span><br><span class="line"></span><br><span class="line"><span class="comment"># Draw path of a record with a sampled datapoint</span></span><br><span class="line">record = <span class="string">'Record008'</span></span><br><span class="line">draw_record(apollo_dataset, record)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>output:</p><p>Alternatively, we can see all records at once in one chart:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Draw all records for current dataset</span></span><br><span class="line">draw_record(apollo_dataset)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>output:</p><p>Another option is to see it in a video:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> utils.common <span class="keyword">import</span> make_video</span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate and save video for the record</span></span><br><span class="line">outfile = <span class="string">"./output_data/videos/video_&#123;&#125;_&#123;&#125;.mp4"</span>.format(apollo_dataset.road, apollo_dataset.record)</span><br><span class="line">make_video(apollo_dataset, outfile=outfile)</span><br></pre></td></tr></table></figure><p>Output (cut gif version of the generated video):</p><p>For the PoseNet training we will use mono images with zero-mean normalized poses and camera images center-cropped to 250px:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Resize and CenterCrop</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize(<span class="number">260</span>),</span><br><span class="line">    transforms.CenterCrop(<span class="number">250</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create train dataset with mono images, normalized poses, enabled cache_transform</span></span><br><span class="line">train_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road=<span class="string">"zpark-sample"</span>,</span><br><span class="line">                             transform=transform, train=<span class="keyword">True</span>, pose_format=<span class="string">'quat'</span>,</span><br><span class="line">                             normalize_poses=<span class="keyword">True</span>, cache_transform=<span class="keyword">True</span>,</span><br><span class="line">                             stereo=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Draw path of a single record (mono with normalized poses)</span></span><br><span class="line">record = <span class="string">'Record008'</span></span><br><span class="line">draw_record(apollo_dataset, record)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>Output:</p><p>Implemented Apolloscape Pytorch dataset also supports cache_transform option which is when enabled saves all transformed pickled images to a disk and retrieves it later for the subsequent epochs without the need to redo convert and transform operations every image read event. Cache saves up to 50% of the time during training time though it’s not working with image augmentation transforms like torchvision.transforms.ColorJitter.</p><p>Also, we can get the mean and the standard deviation that we need later to recover true poses translations:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">poses_mean = train_dataset.poses_mean</span><br><span class="line">poses_std = train_dataset.poses_std</span><br><span class="line">print(<span class="string">'Translation poses_mean = &#123;&#125; in meters'</span>.format(poses_mean))</span><br><span class="line">print(<span class="string">'Translation poses_std  = &#123;&#125; in meters'</span>.format(poses_std))</span><br></pre></td></tr></table></figure><p>Output:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Translation poses_mean = [  <span class="number">449.95782055</span> <span class="number">-2251.24771214</span>    <span class="number">40.17147932</span>] <span class="keyword">in</span> meters</span><br><span class="line">Translation poses_std  = [<span class="number">123.39589457</span> <span class="number">252.42350964</span>   <span class="number">0.28021513</span>] <span class="keyword">in</span> meters</span><br></pre></td></tr></table></figure><p>You can find all mentioned examples in Apolloscape_View_Records.ipynb notebook.</p><p>And now let’s turn to something useful and more interesting, for example, training PoseNet deep convolutional network to regress poses from camera images.</p><h2 id="PoseNet-localization-task"><a href="#PoseNet-localization-task" class="headerlink" title="PoseNet localization task"></a>PoseNet localization task</h2><p>参考：<a href="https://capsulesbot.com/blog/2018/08/24/apolloscape-posenet-pytorch.html" target="_blank" rel="noopener">PoseNet implementation for self-driving car localization using Pytorch on Apolloscape dataset</a></p><p>A Pytorch implementation of the PoseNet model using a mono image:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PoseNet</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, feature_extractor, num_features=<span class="number">128</span>, dropout=<span class="number">0.5</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 track_running_stats=False, pretrained=False)</span>:</span></span><br><span class="line">        super(PoseNet, self).__init__()</span><br><span class="line">        self.dropout = dropout</span><br><span class="line">        self.feature_extractor = feature_extractor</span><br><span class="line">        self.feature_extractor.avgpool = torch.nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        fc_in_features = self.feature_extractor.fc.in_features</span><br><span class="line">        self.feature_extractor.fc = torch.nn.Linear(fc_in_features, num_features)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Translation</span></span><br><span class="line">        self.fc_xyz = torch.nn.Linear(num_features, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Rotation in quaternions</span></span><br><span class="line">        self.fc_quat = torch.nn.Linear(num_features, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extract_features</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x_features = self.feature_extractor(x)</span><br><span class="line">        x_features = F.relu(x_features)</span><br><span class="line">        <span class="keyword">if</span> self.dropout &gt; <span class="number">0</span>:</span><br><span class="line">            x_features = F.dropout(x_features, p=self.dropout, training=self.training)</span><br><span class="line">        <span class="keyword">return</span> x_features</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x_features = self.extract_features(x)</span><br><span class="line">        x_translations = self.fc_xyz(x_features)</span><br><span class="line">        x_rotations = self.fc_quat(x_features)</span><br><span class="line">        x_poses = torch.cat((x_translations, x_rotations), dim=<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x_poses</span><br></pre></td></tr></table></figure><p>For further experiments I’ve also implemented stereo version (currently it’s simply processes two images in parallel without any additional constraints), option to switch off stats tracking for BatchNorm layers and Kaiming He normal for weight initialization [4]. Full source code is here models/posenet.py</p><h2 id="PoseNet-Loss-Functions"><a href="#PoseNet-Loss-Functions" class="headerlink" title="PoseNet Loss Functions"></a>PoseNet Loss Functions</h2><p>For more details on where it came from and intro to Bayesian Deep Learning (BDL) you can refer to an excellent post by Alex Kendall where he explains different types of uncertainties and its implications to the multi-task models. And even more results you can find in papers “Multi-task learning using uncertainty to weigh losses for scene geometry and semantics.” [5] and “What uncertainties do we need in Bayesian deep learning for computer vision?.” [6].</p><p>Pytorch implementation for both versions of a loss function is the following:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PoseNetCriterion</span><span class="params">(torch.nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, beta = <span class="number">512.0</span>, learn_beta=False, sx=<span class="number">0.0</span>, sq=<span class="number">-3.0</span>)</span>:</span></span><br><span class="line">        super(PoseNetCriterion, self).__init__()</span><br><span class="line">        self.loss_fn = torch.nn.L1Loss()</span><br><span class="line">        self.learn_beta = learn_beta</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> learn_beta:</span><br><span class="line">            self.beta = beta</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.beta = <span class="number">1.0</span></span><br><span class="line">        self.sx = torch.nn.Parameter(torch.Tensor([sx]), requires_grad=learn_beta)</span><br><span class="line">        self.sq = torch.nn.Parameter(torch.Tensor([sq]), requires_grad=learn_beta)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        <span class="comment"># Translation loss</span></span><br><span class="line">        loss = torch.exp(-self.sx) * self.loss_fn(x[:, :<span class="number">3</span>], y[:, :<span class="number">3</span>])</span><br><span class="line">        <span class="comment"># Rotation loss</span></span><br><span class="line">        loss += torch.exp(-self.sq) * self.beta * self.loss_fn(x[:, <span class="number">3</span>:], y[:, <span class="number">3</span>:]) + self.sq</span><br><span class="line">        <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure><p>If learn_beta param is False it’s a simple weighted sum version of the loss and if learn_beta is True it’s using sx and sq params with enabled gradients that trains together with other network parameter with the same optimizer.</p><h2 id="PoseNet-Training-Implementation-Details"><a href="#PoseNet-Training-Implementation-Details" class="headerlink" title="PoseNet Training Implementation Details"></a>PoseNet Training Implementation Details</h2><p>Now let’s combine it all to the training loop. I use torch.optim.Adam optimizer with learning rate 1e-5, ResNet34 pretrained on ImageNet as a feature extractor and 2048 features on the last FC layer before pose regressors.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms, models</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset, DataLoader</span><br><span class="line"><span class="keyword">from</span> datasets.apolloscape <span class="keyword">import</span> Apolloscape</span><br><span class="line"><span class="keyword">from</span> utils.common <span class="keyword">import</span> save_checkpoint</span><br><span class="line"><span class="keyword">from</span> models.posenet <span class="keyword">import</span> PoseNet, PoseNetCriterion</span><br><span class="line"></span><br><span class="line">APOLLO_PATH = <span class="string">"./data/apolloscape"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ImageNet normalization params because we are using pre-trained</span></span><br><span class="line"><span class="comment"># feature extractor</span></span><br><span class="line">normalize = transforms.Normalize(mean=[<span class="number">0.485</span>, <span class="number">0.456</span>, <span class="number">0.406</span>],</span><br><span class="line">                                     std=[<span class="number">0.229</span>, <span class="number">0.224</span>, <span class="number">0.225</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Resize data before using</span></span><br><span class="line">transform = transforms.Compose([</span><br><span class="line">    transforms.Resize(<span class="number">260</span>),</span><br><span class="line">    transforms.CenterCrop(<span class="number">250</span>),</span><br><span class="line">    transforms.ToTensor(),</span><br><span class="line">    normalize</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create datasets</span></span><br><span class="line">train_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road=<span class="string">"zpark-sample"</span>,</span><br><span class="line">    transform=transform, normalize_poses=<span class="keyword">True</span>, pose_format=<span class="string">'quat'</span>, train=<span class="keyword">True</span>, cache_transform=<span class="keyword">True</span>, stereo=<span class="keyword">False</span>)</span><br><span class="line">val_dataset = Apolloscape(root=os.path.join(APOLLO_PATH), road=<span class="string">"zpark-sample"</span>,</span><br><span class="line">    transform=transform, normalize_poses=<span class="keyword">True</span>, pose_format=<span class="string">'quat'</span>, train=<span class="keyword">False</span>, cache_transform=<span class="keyword">True</span>, stereo=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Dataloaders</span></span><br><span class="line">train_dataloader = DataLoader(train_dataset, batch_size=<span class="number">80</span>, shuffle=<span class="keyword">True</span>)</span><br><span class="line">val_dataloader = DataLoader(val_dataset, batch_size=<span class="number">80</span>, shuffle=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Select primary device</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device = torch.device(<span class="string">'cuda'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    device = torch.device(<span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create pretrained feature extractor</span></span><br><span class="line">feature_extractor = models.resnet34(pretrained=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Num features for the last layer before pose regressor</span></span><br><span class="line">num_features = <span class="number">2048</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Create model</span></span><br><span class="line">model = PoseNet(feature_extractor, num_features=num_features, pretrained=<span class="keyword">True</span>)</span><br><span class="line">model = model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Criterion</span></span><br><span class="line">criterion = PoseNetCriterion(stereo=<span class="keyword">False</span>, learn_beta=<span class="keyword">True</span>)</span><br><span class="line">criterion = criterion.to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add all params for optimization</span></span><br><span class="line">param_list = [&#123;<span class="string">'params'</span>: model.parameters()&#125;]</span><br><span class="line"><span class="keyword">if</span> criterion.learn_beta:</span><br><span class="line">    <span class="comment"># Add sx and sq from loss function to optimizer params</span></span><br><span class="line">    param_list.append(&#123;<span class="string">'params'</span>: criterion.parameters()&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create optimizer</span></span><br><span class="line">optimizer = optim.Adam(params=param_list, lr=<span class="number">1e-5</span>, weight_decay=<span class="number">0.0005</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Epochs to train</span></span><br><span class="line">n_epochs = <span class="number">2000</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Main training loop</span></span><br><span class="line">val_freq = <span class="number">200</span></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(<span class="number">0</span>, n_epochs):</span><br><span class="line">    train(train_dataloader, model, criterion, optimizer, e, n_epochs, log_freq=<span class="number">0</span>,</span><br><span class="line">         poses_mean=train_dataset.poses_mean, poses_std=train_dataset.poses_std,</span><br><span class="line">         stereo=<span class="keyword">False</span>)</span><br><span class="line">    <span class="keyword">if</span> e % val_freq == <span class="number">0</span>:</span><br><span class="line">        validate(val_dataloader, model, criterion, e, log_freq=<span class="number">0</span>,</span><br><span class="line">            stereo=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save checkpoint</span></span><br><span class="line">save_checkpoint(model, optimizer, criterion, <span class="string">'zpark_experiment'</span>, n_epochs)</span><br></pre></td></tr></table></figure><p>A little bit simplified train function below with error calculation that is used solely for logging purposes:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(train_loader, model, criterion, optimizer, epoch, max_epoch,</span></span></span><br><span class="line"><span class="function"><span class="params">          log_freq=<span class="number">1</span>, print_sum=True, poses_mean=None, poses_std=None,</span></span></span><br><span class="line"><span class="function"><span class="params">          stereo=True)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># switch model to training</span></span><br><span class="line">    model.train()</span><br><span class="line"></span><br><span class="line">    losses = AverageMeter()</span><br><span class="line"></span><br><span class="line">    epoch_time = time.time()</span><br><span class="line"></span><br><span class="line">    gt_poses = np.empty((<span class="number">0</span>, <span class="number">7</span>))</span><br><span class="line">    pred_poses = np.empty((<span class="number">0</span>, <span class="number">7</span>))</span><br><span class="line"></span><br><span class="line">    end = time.time()</span><br><span class="line">    <span class="keyword">for</span> idx, (batch_images, batch_poses) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        data_time = (time.time() - end)</span><br><span class="line"></span><br><span class="line">        batch_images = batch_images.to(device)</span><br><span class="line">        batch_poses = batch_poses.to(device)</span><br><span class="line"></span><br><span class="line">        out = model(batch_images)</span><br><span class="line">        loss = criterion(out, batch_poses)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Training step</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        losses.update(loss.data[<span class="number">0</span>], len(batch_images) * batch_images[<span class="number">0</span>].size(<span class="number">0</span>) <span class="keyword">if</span> stereo</span><br><span class="line">                <span class="keyword">else</span> batch_images.size(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># move data to cpu &amp; numpy</span></span><br><span class="line">        bp = batch_poses.detach().cpu().numpy()</span><br><span class="line">        outp = out.detach().cpu().numpy()</span><br><span class="line">        gt_poses = np.vstack((gt_poses, bp))</span><br><span class="line">        pred_poses = np.vstack((pred_poses, outp))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Get final times</span></span><br><span class="line">        batch_time = (time.time() - end)</span><br><span class="line">        end = time.time()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> log_freq != <span class="number">0</span> <span class="keyword">and</span> idx % log_freq == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">'Epoch: [&#123;&#125;/&#123;&#125;]\tBatch: [&#123;&#125;/&#123;&#125;]\t'</span></span><br><span class="line">                  <span class="string">'Time: &#123;batch_time:.3f&#125;\t'</span></span><br><span class="line">                  <span class="string">'Data Time: &#123;data_time:.3f&#125;\t'</span></span><br><span class="line">                  <span class="string">'Loss: &#123;losses.val:.3f&#125;\t'</span></span><br><span class="line">                  <span class="string">'Avg Loss: &#123;losses.avg:.3f&#125;\t'</span>.format(</span><br><span class="line">                   epoch, max_epoch - <span class="number">1</span>, idx, len(train_loader) - <span class="number">1</span>,</span><br><span class="line">                   batch_time=batch_time, data_time=data_time, losses=losses))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># un-normalize translation</span></span><br><span class="line">    unnorm = (poses_mean <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>) <span class="keyword">and</span> (poses_std <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>)</span><br><span class="line">    <span class="keyword">if</span> unnorm:</span><br><span class="line">        gt_poses[:, :<span class="number">3</span>] = gt_poses[:, :<span class="number">3</span>] * poses_std + poses_mean</span><br><span class="line">        pred_poses[:, :<span class="number">3</span>] = pred_poses[:, :<span class="number">3</span>] * poses_std + poses_mean</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Translation error</span></span><br><span class="line">    t_loss = np.asarray([np.linalg.norm(p - t) <span class="keyword">for</span> p, t <span class="keyword">in</span> zip(pred_poses[:, :<span class="number">3</span>], gt_poses[:, :<span class="number">3</span>])])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Rotation error</span></span><br><span class="line">    q_loss = np.asarray([quaternion_angular_error(p, t) <span class="keyword">for</span> p, t <span class="keyword">in</span> zip(pred_poses[:, <span class="number">3</span>:], gt_poses[:, <span class="number">3</span>:])])</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> print_sum:</span><br><span class="line">        print(<span class="string">'Ep: [&#123;&#125;/&#123;&#125;]\tTrain Loss: &#123;:.3f&#125;\tTe: &#123;:.3f&#125;\tRe: &#123;:.3f&#125;\t Et: &#123;:.2f&#125;s\t&#123;criterion_sx:.5f&#125;:&#123;criterion_sq:.5f&#125;'</span>.format(</span><br><span class="line">            epoch, max_epoch - <span class="number">1</span>, losses.avg, np.mean(t_loss), np.mean(q_loss),</span><br><span class="line">            (time.time() - epoch_time), criterion_sx=criterion.sx.data[<span class="number">0</span>], criterion_sq=criterion.sq.data[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure><p>validate function is similar to train except model.eval()/model.train() modes, logging and error calculations. Please refer to /utils/training.py on GitHub for full-versions of train and validate functions.</p><p>The training converges after about 1-2k epochs. On my machine, with GTX 1080 Ti it takes about 22 seconds per epoch on ZPark sample train dataset with 2242 images pre-processed and scaled to 250x250 pixels. Total training time – 6-12 hours.</p><h2 id="PoseNet-Results-on-Apolloscape-dataset-ZPark-sample-road"><a href="#PoseNet-Results-on-Apolloscape-dataset-ZPark-sample-road" class="headerlink" title="PoseNet Results on Apolloscape dataset. ZPark sample road."></a>PoseNet Results on Apolloscape dataset. ZPark sample road.</h2><p>After 2k epochs of training, the model was managed to get a prediction of pose translation with a mean 40.6 meters and rotation with a mean 1.69 degrees.</p><h2 id="Further-development"><a href="#Further-development" class="headerlink" title="Further development"></a>Further development</h2><p>Established results are far from one that can be used in autonomous navigation where a system needs to now its location within accuracy of 15cm. Such precision is vital for a car to act safely, correctly predict the behaviors of others and plan actions accordingly. In any case, it’s a good baseline and building blocks of the pipeline to work with Apolloscape dataset that I can develop and improve further.</p><p>There many things to try next:</p><ul><li>Use temporal nature of a video.</li><li>Rely on geometrical features of stereo cameras.</li><li>Pose graph optimization techniques.</li><li>Loss based on 3D reprojection errors.</li><li>Structure from motion methods to build 3D map representation.</li></ul><p>And what’s more importantly, all above-mentioned methods need no additional information but that we already have in ZPark sample road from Apolloscape dataset.</p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul><li><ol><li>Kendall, Alex, and Roberto Cipolla. “Geometric loss functions for camera pose regression with deep learning.” (2017).</li></ol></li><li><ol start="2"><li>Kendall, Alex, Matthew Grimes, and Roberto Cipolla. “Posenet: A convolutional network for real-time 6-dof camera relocalization.” (2015).</li></ol></li><li><ol start="3"><li>Brahmbhatt, Samarth, et al. “Mapnet: Geometry-aware learning of maps for camera localization.” (2017).</li></ol></li><li><ol start="4"><li>He, Kaiming, et al. “Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.” (2015).</li></ol></li><li><ol start="5"><li>Kendall, Alex, Yarin Gal, and Roberto Cipolla. “Multi-task learning using uncertainty to weigh losses for scene geometry and semantics.” (2017).</li></ol></li><li><ol start="6"><li>Kendall, Alex, and Yarin Gal. “What uncertainties do we need in bayesian deep learning for computer vision?.” (2017).</li></ol></li><li><ol start="7"><li>Clark, Ronald, et al. “VidLoc: A deep spatio-temporal model for 6-dof video-clip relocalization.” (2017).</li></ol></li><li><ol start="8"><li>Calafiore, Giuseppe, Luca Carlone, and Frank Dellaert. “Pose graph optimization in the complex domain: Lagrangian duality, conditions for zero duality gap, and optimal solutions.” (2015).</li></ol></li><li><ol start="9"><li>Martinec, Daniel, and Tomas Pajdla. “Robust rotation and translation estimation in multiview reconstruction.” (2007).</li></ol></li></ul><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ul><li><a href="https://capsulesbot.com/blog/2018/08/24/apolloscape-posenet-pytorch.html" target="_blank" rel="noopener">PoseNet implementation for self-driving car localization using Pytorch on Apolloscape dataset</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
          <category> 竞赛 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
            <tag> 竞赛 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>AWS免费搭建ss教程</title>
      <link href="/2018/06/11/AWS%E5%85%8D%E8%B4%B9%E6%90%AD%E5%BB%BAss%E6%95%99%E7%A8%8B/"/>
      <url>/2018/06/11/AWS%E5%85%8D%E8%B4%B9%E6%90%AD%E5%BB%BAss%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="注册免费的AWS服务器"><a href="#注册免费的AWS服务器" class="headerlink" title="注册免费的AWS服务器"></a>注册免费的AWS服务器</h1><p>注册地址：<a href="https://aws.amazon.com/cn/ec2/pricing/?sc_channel=PS&amp;sc_campaign=acquisition_CN&amp;sc_publisher=baidu&amp;sc_category=pc&amp;sc_medium=ec2_b&amp;sc_content=ec2_e&amp;sc_detail=ec2&amp;sc_segment=100009040&amp;sc_matchtype=phrase&amp;sc_country=CN&amp;s_kwcid=AL!4422!88!5666095499!!17798814222&amp;ef_id=WjtnMQAAAdinHin@:20180611133737:s" target="_blank" rel="noopener">ec2 免费试用全球云服务-亚马逊AWS</a></p><p>填写注册信息、输入信用卡（成功后扣除1美元）、电话语音确认。</p><p>上述过程成功后，即可进入EC2创建服务器实例。如下图所示：</p><p><img src="http://p55se4hrx.bkt.clouddn.com/images/programming/AWS_SS1.jpeg" alt></p><h1 id="创建服务器实例"><a href="#创建服务器实例" class="headerlink" title="创建服务器实例"></a>创建服务器实例</h1><p>选择Ubuntu16.04 LTS系统：</p><p><img src="http://p55se4hrx.bkt.clouddn.com/images/programming/AWS_SS2.jpeg" alt></p><p>选择实例类型：</p><p><img src="http://p55se4hrx.bkt.clouddn.com/images/programming/AWS_SS3.jpeg" alt></p><p>根据自己的需求添加存储，免费用户最大30GB存储：</p><p><img src="http://p55se4hrx.bkt.clouddn.com/images/programming/AWS_SS4.jpeg" alt></p><p>创建秘钥对：</p><p><img src="http://p55se4hrx.bkt.clouddn.com/images/programming/AWS_SS5.jpeg" alt></p><p>编辑安全组规则：</p><p><img src="http://p55se4hrx.bkt.clouddn.com/images/programming/AWS_SS6.jpeg" alt></p><h1 id="登录实例并配置ss环境"><a href="#登录实例并配置ss环境" class="headerlink" title="登录实例并配置ss环境"></a>登录实例并配置ss环境</h1><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo chmod 400 XXX.pem <span class="comment"># 更改秘钥权限</span></span><br><span class="line">$ ssh -i XXX.pem ubuntu@[IP] <span class="comment"># 利用公有IP登录实例</span></span><br></pre></td></tr></table></figure><p>安装Shadowsocks:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获取root权限</span></span><br><span class="line">sudo -s</span><br><span class="line"><span class="comment"># 更新apt-get</span></span><br><span class="line">apt-get update</span><br><span class="line"><span class="comment"># 安装python包管理工具</span></span><br><span class="line">apt-get install python-setuptools</span><br><span class="line">apt-get install python-pip</span><br><span class="line"><span class="comment"># 安装shadowsocks</span></span><br><span class="line">pip install shadowsocks</span><br></pre></td></tr></table></figure><p>创建配置文件:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /etc/shadowsocks</span><br><span class="line">vim /etc/shadowsocks/ss.json</span><br></pre></td></tr></table></figure><p>配置文件内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="string">"server"</span>:<span class="string">"0.0.0.0"</span>,</span><br><span class="line">    <span class="string">"server_port"</span>:443, //ss连接服务器的端口</span><br><span class="line">    <span class="string">"local_address"</span>:<span class="string">"127.0.0.1"</span>,</span><br><span class="line">    <span class="string">"local_port"</span>:1080,</span><br><span class="line">    <span class="string">"password"</span>:<span class="string">"abcd1234"</span>, // 设置ss连接时的密码</span><br><span class="line">    <span class="string">"timeout"</span>:300,</span><br><span class="line">    <span class="string">"method"</span>:<span class="string">"aes-256-cfb"</span>,</span><br><span class="line">    <span class="string">"fast_open"</span>:<span class="literal">false</span>,</span><br><span class="line">    <span class="string">"workers"</span>: 1</span><br><span class="line">&#125;/Users/xiaoqiangteng/programmings/growing-up/xiaoqiangteng.github.io/<span class="built_in">source</span>/_posts/vultr的ss服务器教程.md</span><br></pre></td></tr></table></figure><p>启动Shadowsocks:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">启动：sudo /usr/<span class="built_in">local</span>/bin/ssserver -c /etc/shadowsocks/ss.json -d start</span><br><span class="line"></span><br><span class="line">停止：sudo /usr/<span class="built_in">local</span>/bin/ssserver -c /etc/shadowsocks/ss.json -d stop  </span><br><span class="line">重启：sudo /usr/<span class="built_in">local</span>/bin/ssserver -c /etc/shadowsocks/ss.json -d restart</span><br></pre></td></tr></table></figure><h1 id="配置自动切换模式"><a href="#配置自动切换模式" class="headerlink" title="配置自动切换模式"></a>配置自动切换模式</h1><p>配置好 ss 情景模式后虽然可以使用 Chrome 浏览器科学上网了，但是这样的话无论你访问什么网站都会走代理，有时候访问国内的一些网站反而会很慢，这时候自动切换模式就解决了这个问题。下面介绍一下如何配置自动切换模式。</p><p>点击左侧的 自动切换，或者自己新建情景模式，类型选择第二个 自动切换模式。然后做如下配置：</p><p><img src="http://p55se4hrx.bkt.clouddn.com/images/programming/AWS_SS7.jpeg" alt></p><p>导入在线规则列表，类型选择AutoProxy，可以选择导入gfwlist - <a href="https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt" target="_blank" rel="noopener">https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt</a> 或者自己自定义的AutoProxy文件。</p><p>保存设置并更新情景模式，若更新失败则开启全局代理后更新。</p><p>设置规则匹配则使用代理模式，否则直接连接。保存退出。</p><h1 id="参考："><a href="#参考：" class="headerlink" title="参考："></a>参考：</h1><ul><li><a href="https://blog.csdn.net/miracleswang/article/details/78959305" target="_blank" rel="noopener">https://blog.csdn.net/miracleswang/article/details/78959305</a></li><li><a href="https://blog.csdn.net/kntanchao/article/details/79191149" target="_blank" rel="noopener">https://blog.csdn.net/kntanchao/article/details/79191149</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
          <category> AWS </category>
          
          <category> SS </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
            <tag> AWS </tag>
            
            <tag> ss </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习-TensorFlow教程</title>
      <link href="/2018/03/30/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-TensorFlow%E6%95%99%E7%A8%8B/"/>
      <url>/2018/03/30/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0-TensorFlow%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="TensorFlow简介"><a href="#TensorFlow简介" class="headerlink" title="TensorFlow简介"></a>TensorFlow简介</h1><p>TensorFlow是Google开发的一款神经网络的Python外部的结构包, 也是一个采用数据流图来进行数值计算的开源软件库.TensorFlow 让我们可以先绘制计算结构图, 也可以称是一系列可人机交互的计算操作, 然后把编辑好的Python文件 转换成 更高效的C++, 并在后端进行计算.</p><h1 id="TensorFlow安装"><a href="#TensorFlow安装" class="headerlink" title="TensorFlow安装"></a>TensorFlow安装</h1><h2 id="Docker安装"><a href="#Docker安装" class="headerlink" title="Docker安装"></a>Docker安装</h2><h3 id="Dock安装"><a href="#Dock安装" class="headerlink" title="Dock安装"></a>Dock安装</h3><p>Docker安装请参考<a href="http://mapstec.com/2018/03/29/%E5%AE%9E%E9%AA%8C%E5%AE%A4GPU%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%83%A8%E7%BD%B2%E6%95%99%E7%A8%8B/" target="_blank" rel="noopener">实验室GPU服务器部署教程</a></p><p>Docker 需要用户具有 sudo 权限，为了避免每次命令都输入sudo，可以把用户加入 Docker 用户组，参考：<a href="https://docs.docker.com/install/linux/linux-postinstall/#manage-docker-as-a-non-root-user" target="_blank" rel="noopener">docker docs</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo usermod -aG docker <span class="variable">$USER</span></span><br></pre></td></tr></table></figure><h3 id="安装NVIDIA-Docker"><a href="#安装NVIDIA-Docker" class="headerlink" title="安装NVIDIA-Docker"></a>安装NVIDIA-Docker</h3><p>安装完成docker并检查安装正确（能跑出来hello-world）后，如果需要docker容器中有gpu支持，需要再安装NVIDIA-Docker，同样找到并打开该项目的主页：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NVIDIA/nvidia-docker: Build and run Docker containers leveraging NVIDIA GPUs</span><br><span class="line">https://github.com/NVIDIA/nvidia-docker</span><br></pre></td></tr></table></figure><p>可以看到在Quick start小节，根据系统版本执行命令：</p><p>Ubuntu 14.04/16.04/18.04, Debian Jessie/Stretch：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If you have nvidia-docker 1.0 installed: we need to remove it and all existing GPU containers</span></span><br><span class="line">docker volume ls -q -f driver=nvidia-docker | xargs -r -I&#123;&#125; -n1 docker ps -q -a -f volume=&#123;&#125; | xargs -r docker rm -f</span><br><span class="line">sudo apt-get purge -y nvidia-docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add the package repositories</span></span><br><span class="line">curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | \</span><br><span class="line">  sudo apt-key add -</span><br><span class="line">distribution=$(. /etc/os-release;<span class="built_in">echo</span> <span class="variable">$ID</span><span class="variable">$VERSION_ID</span>)</span><br><span class="line">curl -s -L https://nvidia.github.io/nvidia-docker/<span class="variable">$distribution</span>/nvidia-docker.list | \</span><br><span class="line">  sudo tee /etc/apt/sources.list.d/nvidia-docker.list</span><br><span class="line">sudo apt-get update</span><br><span class="line"></span><br><span class="line"><span class="comment"># Install nvidia-docker2 and reload the Docker daemon configuration</span></span><br><span class="line">sudo apt-get install -y nvidia-docker2</span><br><span class="line">sudo pkill -SIGHUP dockerd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test nvidia-smi with the latest official CUDA image</span></span><br><span class="line">docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi</span><br></pre></td></tr></table></figure><p>CentOS 7 (docker-ce), RHEL 7.4/7.5 (docker-ce), Amazon Linux 1/2:</p><p>If you are not using the official docker-ce package on CentOS/RHEL, use the next section.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If you have nvidia-docker 1.0 installed: we need to remove it and all existing GPU containers</span></span><br><span class="line">docker volume ls -q -f driver=nvidia-docker | xargs -r -I&#123;&#125; -n1 docker ps -q -a -f volume=&#123;&#125; | xargs -r docker rm -f</span><br><span class="line">sudo yum remove nvidia-docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add the package repositories</span></span><br><span class="line">distribution=$(. /etc/os-release;<span class="built_in">echo</span> <span class="variable">$ID</span><span class="variable">$VERSION_ID</span>)</span><br><span class="line">curl -s -L https://nvidia.github.io/nvidia-docker/<span class="variable">$distribution</span>/nvidia-docker.repo | \</span><br><span class="line">  sudo tee /etc/yum.repos.d/nvidia-docker.repo</span><br><span class="line"></span><br><span class="line"><span class="comment"># Install nvidia-docker2 and reload the Docker daemon configuration</span></span><br><span class="line">sudo yum install -y nvidia-docker2</span><br><span class="line">sudo pkill -SIGHUP dockerd</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test nvidia-smi with the latest official CUDA image</span></span><br><span class="line">docker run --runtime=nvidia --rm nvidia/cuda nvidia-smi</span><br></pre></td></tr></table></figure><p>If yum reports a conflict on /etc/docker/daemon.json with the docker package, you need to use the next section instead.</p><p>CentOS 7 (docker), RHEL 7.4/7.5 (docker):</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># If you have nvidia-docker 1.0 installed: we need to remove it and all existing GPU containers</span></span><br><span class="line">docker volume ls -q -f driver=nvidia-docker | xargs -r -I&#123;&#125; -n1 docker ps -q -a -f volume=&#123;&#125; | xargs -r docker rm -f</span><br><span class="line">sudo yum remove nvidia-docker</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add the package repositories</span></span><br><span class="line">distribution=$(. /etc/os-release;<span class="built_in">echo</span> <span class="variable">$ID</span><span class="variable">$VERSION_ID</span>)</span><br><span class="line">curl -s -L https://nvidia.github.io/nvidia-container-runtime/<span class="variable">$distribution</span>/nvidia-container-runtime.repo | \</span><br><span class="line">  sudo tee /etc/yum.repos.d/nvidia-container-runtime.repo</span><br><span class="line"></span><br><span class="line"><span class="comment"># Install the nvidia runtime hook</span></span><br><span class="line">sudo yum install -y nvidia-container-runtime-hook</span><br><span class="line">sudo mkdir -p /usr/libexec/oci/hooks.d</span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">'#!/bin/sh\nPATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin" exec nvidia-container-runtime-hook "$@"'</span> | \</span><br><span class="line">  sudo tee /usr/libexec/oci/hooks.d/nvidia</span><br><span class="line">sudo chmod +x /usr/libexec/oci/hooks.d/nvidia</span><br><span class="line"></span><br><span class="line"><span class="comment"># Test nvidia-smi with the latest official CUDA image</span></span><br><span class="line"><span class="comment"># You can't use `--runtime=nvidia` with this setup.</span></span><br><span class="line">docker run --rm nvidia/cuda nvidia-smi</span><br></pre></td></tr></table></figure><p>上面最后一条命令是检查是否安装成功，安装成功，则会显示关于GPU的信息。</p><p>然后在执行下面这句，默认用nvdia-docker替代docker命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">'alias docker=nvidia-docker'</span> &gt;&gt; ~/.bashrc</span><br><span class="line">bash</span><br></pre></td></tr></table></figure><h3 id="下载使用TensorFlow镜像"><a href="#下载使用TensorFlow镜像" class="headerlink" title="下载使用TensorFlow镜像"></a>下载使用TensorFlow镜像</h3><p>打开dockerhub关于tensorflow的页面：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tensorflow/tensorflow – Docker Hub</span><br><span class="line">https://hub.docker.com/r/tensorflow/tensorflow/</span><br></pre></td></tr></table></figure><p>根据需要的版本下载tensorflow镜像并开启tensorflow容器：</p><p>CPU版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it -p 8888:8888 tensorflow/tensorflow</span><br></pre></td></tr></table></figure><p>GPU版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-docker run -it -p 8888:8888 tensorflow/tensorflow:latest-gpu</span><br></pre></td></tr></table></figure><p>如何使用,执行以上命令的结果类似如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ nvidia-docker run -it -p 8888:8888 tensorflow/tensorflow:latest-gpu</span><br><span class="line">[I 02:51:21.230 NotebookApp] Writing notebook server cookie secret to /root/.<span class="built_in">local</span>/share/jupyter/runtime/notebook_cookie_secret</span><br><span class="line">[W 02:51:21.242 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.</span><br><span class="line">[I 02:51:21.249 NotebookApp] Serving notebooks from <span class="built_in">local</span> directory: /notebooks</span><br><span class="line">[I 02:51:21.249 NotebookApp] 0 active kernels </span><br><span class="line">[I 02:51:21.249 NotebookApp] The Jupyter Notebook is running at: http://[all ip addresses on your system]:8888/?token=8f90cc7b9ad6ccc4f36f53f347c7a314220bbcb82dd416ea</span><br><span class="line">[I 02:51:21.249 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).</span><br><span class="line">[C 02:51:21.249 NotebookApp] </span><br><span class="line">    </span><br><span class="line">    Copy/paste this URL into your browser when you connect <span class="keyword">for</span> the first time,</span><br><span class="line">    to login with a token:</span><br><span class="line">        http://localhost:8888/?token=8f90cc7b9ad6ccc4f36f53f347c7a314220bbcb82dd416ea</span><br><span class="line">[I 02:51:31.832 NotebookApp] 302 GET / (172.17.0.1) 0.74ms</span><br><span class="line">[I 02:51:31.943 NotebookApp] 302 GET /tree? (172.17.0.1) 1.44ms</span><br></pre></td></tr></table></figure><p>其中看到有个网址：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://localhost:8888/?token=8f90cc7b9ad6ccc4f36f53f347c7a314220bbcb82dd416ea</span><br></pre></td></tr></table></figure><p>每个人的网址在token=后面的内容是不一样的，现在我们打开浏览器，输入网址：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://localhost:8888/</span><br></pre></td></tr></table></figure><p>输入刚刚token后面的值后,点击第一个1_hello_tensorflow.ipynb，然后可以选择执行所有代码.</p><p>常用命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker image pull library/hello-world</span><br></pre></td></tr></table></figure><p>上面代码中，docker image pull是抓取 image 文件的命令。library/hello-world是 image 文件在仓库里面的位置，其中library是 image 文件所在的组，hello-world是 image 文件的名字。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker image ls</span><br></pre></td></tr></table></figure><p>抓取成功以后，就可以在本机看到这个 image 文件了。运行这个 image 文件:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker container run hello-world</span><br></pre></td></tr></table></figure><p>docker container run命令会从 image 文件，生成一个正在运行的容器实例。</p><p>注意，docker container run命令具有自动抓取 image 文件的功能。如果发现本地没有指定的 image 文件，就会从仓库自动抓取。因此，前面的docker image pull命令并不是必需的步骤。</p><p>有些容器不会自动终止，因为提供的是服务。比如，安装运行 Ubuntu 的 image，就可以在命令行体验 Ubuntu 系统。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker container run -it ubuntu bash</span><br></pre></td></tr></table></figure><p>对于那些不会自动终止的容器，必须使用docker container kill 命令手动终止。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker container <span class="built_in">kill</span> [containID]</span><br></pre></td></tr></table></figure><p>image 文件生成的容器实例，本身也是一个文件，称为容器文件。也就是说，一旦容器生成，就会同时存在两个文件： image 文件和容器文件。而且关闭容器并不会删除容器文件，只是容器停止运行而已。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 列出本机正在运行的容器</span></span><br><span class="line">$ docker container ls</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出本机所有容器，包括终止运行的容器</span></span><br><span class="line">$ docker container ls --all</span><br></pre></td></tr></table></figure><p>上面命令的输出结果之中，包括容器的 ID。很多地方都需要提供这个 ID，比如上一节终止容器运行的docker container kill命令。</p><p>终止运行的容器文件，依然会占据硬盘空间，可以使用docker container rm命令删除。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker container rm [containerID]</span><br></pre></td></tr></table></figure><p>运行上面的命令之后，再使用docker container ls –all命令，就会发现被删除的容器文件已经消失了。</p><p>创建tensorflow docker容器：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker container run --name [name] -it -p 8888:8888 tensorflow/tensorflow:latest-gpu /bin/bash</span><br><span class="line"><span class="comment"># [name]-- 容器的名字</span></span><br><span class="line"><span class="comment"># -it -- 保留命令行运行</span></span><br><span class="line"><span class="comment"># -p 8888:8888 —— 将本地的8888端口和http://localhost:8888/映射</span></span><br><span class="line"><span class="comment"># tensorflow/tensorflow:latest-gpu ：默认是tensorflow/tensorflow:latest,指定使用的镜像</span></span><br></pre></td></tr></table></figure><p>启动docker：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker start [name]</span><br></pre></td></tr></table></figure><p>进入docker:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker attach [name]</span><br></pre></td></tr></table></figure><p>重命名docker:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker rename  old_name new_name</span><br></pre></td></tr></table></figure><h3 id="如何进入正在执行的-docker-container"><a href="#如何进入正在执行的-docker-container" class="headerlink" title="如何进入正在执行的 docker container"></a>如何进入正在执行的 docker container</h3><h4 id="docker-attach"><a href="#docker-attach" class="headerlink" title="docker attach"></a>docker attach</h4><p>这个是官方提供的一种方法。</p><p>测试，首先启动一个container:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -i -t ubuntu bash</span><br><span class="line">root@4556f5ad6067:/<span class="comment">#</span></span><br></pre></td></tr></table></figure><p>不要退出，打开另一个终端：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES</span><br><span class="line">4556f5ad6067        ubuntu:14.04        <span class="string">"bash"</span>              45 seconds ago      Up 43 seconds                           jolly_ardinghelli</span><br><span class="line"></span><br><span class="line">$ docker attach 4556f5ad6067</span><br><span class="line">root@4556f5ad6067:/<span class="comment">#</span></span><br></pre></td></tr></table></figure><p>这样就连接进去了。这时候如果我们输入一些命令，就能看到在两个终端都有显示和输出。这种方式有比较大的局限性，如果知道了entrypoint或者有程序正在执行，通过docker attach进入之后是不能执行操作的，一个终端退出之后整个container就终止了。不推荐使用这种方式。</p><h4 id="lxc-attach"><a href="#lxc-attach" class="headerlink" title="lxc-attach"></a>lxc-attach</h4><p>如果使用这种方式，首先要保证docker是以lxc方式启动的，具体可以这样做：</p><p>修改/etc/default/docker增加DOCKER_OPTS=”-e lxc”</p><p>重启docker服务sudo service docker restart</p><p>启动container的方式和之前一样：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -i -t ubuntu bash</span><br><span class="line">root@e7f01f0ff598:/<span class="comment">#</span></span><br></pre></td></tr></table></figure><p>进入container可以这样：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ docker ps</span><br><span class="line">CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES</span><br><span class="line">e7f01f0ff598        ubuntu:14.04        <span class="string">"bash"</span>              17 seconds ago      Up 15 seconds                           grave_jones</span><br><span class="line"></span><br><span class="line">$ ps aux | grep e7f01f0ff598</span><br><span class="line">root     23691  0.0  0.0  43140  1876 pts/9    Ss   21:47   0:00 lxc-start -n e7f01f0ff598c80d70a996135c98fbaeddc6daa61436bbbfa735233e8b6f8ebe -f /var/lib/docker/containers/e7f01f0ff598c80d70a996135c98fbaeddc6daa61436bbbfa735233e8b6f8ebe/config.lxc -- /.dockerinit -g 172.17.42.1 -i 172.17.0.3/16 -mtu 1500 -- bash</span><br><span class="line">ma6174   23756  0.0  0.0  13428   928 pts/12   S+   21:47   0:00 grep --color=auto e7f01f0ff598</span><br><span class="line"></span><br><span class="line">$ sudo lxc-attach -n e7f01f0ff598c80d70a996135c98fbaeddc6daa61436bbbfa735233e8b6f8ebe</span><br><span class="line">root@e7f01f0ff598:/<span class="comment">#</span></span><br></pre></td></tr></table></figure><p>这种方式还是很方便的。前提是需要重启docker服务以lxc的方式执行，进入container之后会有一个终端可以执行命令，不影响正在执行的程序。</p><h4 id="nsenter"><a href="#nsenter" class="headerlink" title="nsenter"></a>nsenter</h4><p>如果docker不是以lxc方式启动的，这时候还想进入一个正在执行的container的话，可以考虑使用nsenter</p><p>这个程序的安装方式很独特，使用docker进行安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm -v /usr/<span class="built_in">local</span>/bin:/target jpetazzo/nsenter</span><br></pre></td></tr></table></figure><p>使用方法也很简单，首先你要进入的container的PID：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ PID=$(docker inspect --format &#123;&#123;.State.Pid&#125;&#125; &lt;container_name_or_ID&gt;)</span><br></pre></td></tr></table></figure><p>然后就可以用这个命令进入container了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nsenter --target <span class="variable">$PID</span> --mount --uts --ipc --net --pid</span><br></pre></td></tr></table></figure><p>为了使用方便可以写一个脚本自动完成：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ cat /bin/docker_enter</span><br><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line">sudo nsenter --target `docker inspect --format &#123;&#123;.State.Pid&#125;&#125; <span class="variable">$1</span>` --mount --uts --ipc --net --pid bash</span><br></pre></td></tr></table></figure><p>这样每次要进入某个container只需要执行docker_enter &lt;container_name_or_ID&gt;就可以了。</p><h4 id="ssh"><a href="#ssh" class="headerlink" title="ssh"></a>ssh</h4><p>这个原理也很简单，在container里面启动ssh服务，然后通过ssh的方式去登陆到container里面，不推荐这种方式，主要是配置ssh登陆比较繁琐，开启ssh服务也会耗费资源，完全没有必要。</p><h3 id="TensorFlow安装方式一"><a href="#TensorFlow安装方式一" class="headerlink" title="TensorFlow安装方式一"></a>TensorFlow安装方式一</h3><ol><li><p>下载镜像</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull tensorflow/tensorflow</span><br></pre></td></tr></table></figure></li><li><p>创建Tensorflow容器</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run --name my-tensorflow -it -p 8888:8888 -v ~/tensorflow:/<span class="built_in">test</span>/data tensorflow/tensorflow</span><br><span class="line"><span class="comment"># --name：创建的容器名，即my-tensorflow</span></span><br><span class="line"><span class="comment"># -it：保留命令行运行</span></span><br><span class="line"><span class="comment"># p 8888:8888：将本地的8888端口和http://localhost:8888/映射</span></span><br><span class="line"><span class="comment"># -v ~/tensorflow:/test/data:将本地的~/tensorflow挂载到容器内的/# test/data下</span></span><br><span class="line"><span class="comment"># tensorflow/tensorflow ：默认是tensorflow/tensorflow:latest,指定使用的镜像</span></span><br></pre></td></tr></table></figure></li><li><p>拷贝带token的URL在浏览器打开</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://[all ip addresses on your system]:8888/?token=649d7cab1734e01db75b6c2b476ea87aa0b24dde56662a27</span><br></pre></td></tr></table></figure></li><li><p>显示Jupyter Notebook，Jupyter Notebook（此前被称为 IPython notebook）是一个交互式笔记本。示例中已经显示了Tensorflow的入门教程，点开一个可以看见。</p></li><li><p>关闭容器</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker stop my-tensortflow</span><br></pre></td></tr></table></figure></li><li><p>再次打开</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker start my-tensortflow</span><br></pre></td></tr></table></figure></li></ol><h3 id="TensorFlow安装方式二"><a href="#TensorFlow安装方式二" class="headerlink" title="TensorFlow安装方式二"></a>TensorFlow安装方式二</h3><ol><li><p>下载镜像</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker pull tensorflow/tensorflow</span><br></pre></td></tr></table></figure></li><li><p>创建Tensorflow容器</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --name bash_tensorflow tensorflow/tensorflow /bin/bash</span><br><span class="line"><span class="comment"># 这样我们就创建了名为bash_tensorflow的容器</span></span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>start命令启动容器</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker start bash_tensorflow</span><br></pre></td></tr></table></figure></li><li><p>再连接上容器</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">docker attach bash_tensorflow</span><br><span class="line"><span class="comment"># 可以看到我们用终端连接上了容器，和操作Linux一样了。</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="Pip安装"><a href="#Pip安装" class="headerlink" title="Pip安装"></a>Pip安装</h2><h3 id="Linux-和-MacOS"><a href="#Linux-和-MacOS" class="headerlink" title="Linux 和 MacOS"></a>Linux 和 MacOS</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Ubuntu/Linux 64-位 系统的执行代码:</span></span><br><span class="line">$ sudo apt-get install python-pip python-dev</span><br><span class="line"></span><br><span class="line"><span class="comment"># Mac OS X 系统的执行代码:</span></span><br><span class="line">$ sudo easy_install --upgrade pip</span><br><span class="line">$ sudo easy_install --upgrade six</span><br></pre></td></tr></table></figure><ol><li><p>CPU 版</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># python 2+ 的用户:</span></span><br><span class="line">$ pip install tensorflow</span><br><span class="line"></span><br><span class="line"><span class="comment"># python 3+ 的用户:</span></span><br><span class="line">$ pip3 install tensorflow</span><br></pre></td></tr></table></figure></li><li><p>GPU 版</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get install libcupti-dev</span><br><span class="line">$ sudo apt-get install python-pip python-dev   <span class="comment"># for Python 2.7</span></span><br><span class="line">$ sudo apt-get install python3-pip python3-dev <span class="comment"># for Python 3.n</span></span><br><span class="line">$ pip install tensorflow      <span class="comment"># Python 2.7; CPU support (no GPU support)</span></span><br><span class="line">$ pip3 install tensorflow     <span class="comment"># Python 3.n; CPU support (no GPU support)</span></span><br><span class="line">$ pip install tensorflow-gpu  <span class="comment"># Python 2.7;  GPU support</span></span><br><span class="line">$ pip3 install tensorflow-gpu <span class="comment"># Python 3.n; GPU support</span></span><br></pre></td></tr></table></figure></li></ol><ol start="3"><li><p>测试</p> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow</span><br></pre></td></tr></table></figure></li></ol><h1 id="TensorFlow-教程"><a href="#TensorFlow-教程" class="headerlink" title="TensorFlow 教程"></a>TensorFlow 教程</h1><h2 id="Session-会话控制"><a href="#Session-会话控制" class="headerlink" title="Session 会话控制"></a>Session 会话控制</h2><p>参考：</p><ul><li><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/2-3-session/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/2-3-session/</a></li></ul><p>Session 是 Tensorflow 为了控制,和输出文件的执行的语句. 运行 session.run() 可以获得你要得知的运算结果, 或者是你所要运算的部分.</p><p>例子讲解：建立两个 matrix ,输出两个 matrix 矩阵相乘的结果。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment"># create two matrixes</span></span><br><span class="line"></span><br><span class="line">matrix1 = tf.constant([[<span class="number">3</span>,<span class="number">3</span>]])</span><br><span class="line">matrix2 = tf.constant([[<span class="number">2</span>],</span><br><span class="line">                       [<span class="number">2</span>]])</span><br><span class="line">product = tf.matmul(matrix1,matrix2)</span><br></pre></td></tr></table></figure><p>因为 product 不是直接计算的步骤, 所以我们会要使用 Session 来激活 product 并得到计算结果. 有两种形式使用会话控制 Session 。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># method 1</span></span><br><span class="line">sess = tf.Session()</span><br><span class="line">result = sess.run(product)</span><br><span class="line">print(result)</span><br><span class="line">sess.close()</span><br><span class="line"><span class="comment"># [[12]]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># method 2</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    result2 = sess.run(product)</span><br><span class="line">    print(result2)</span><br><span class="line"><span class="comment"># [[12]]</span></span><br></pre></td></tr></table></figure><h2 id="Variable-变量"><a href="#Variable-变量" class="headerlink" title="Variable 变量"></a>Variable 变量</h2><p>参考：</p><ul><li><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/2-4-variable/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/2-4-variable/</a></li></ul><p>在 Tensorflow 中，定义了某字符串是变量，它才是变量。定义语法： state = tf.Variable()</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line">state = tf.Variable(<span class="number">0</span>, name=<span class="string">'counter'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义常量 one</span></span><br><span class="line">one = tf.constant(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义加法步骤 (注: 此步并没有直接计算)</span></span><br><span class="line">new_value = tf.add(state, one)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将 State 更新成 new_value</span></span><br><span class="line">update = tf.assign(state, new_value)</span><br></pre></td></tr></table></figure><p>如果你在 Tensorflow 中设定了变量，那么初始化变量是最重要的！！所以定义了变量以后, 一定要定义 init = tf.initialize_all_variables() .</p><p>到这里变量还是没有被激活，需要再在 sess 里, sess.run(init) , 激活 init 这一步.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果定义 Variable, 就一定要 initialize</span></span><br><span class="line"><span class="comment"># init = tf.initialize_all_variables() # tf 马上就要废弃这种写法</span></span><br><span class="line">init = tf.global_variables_initializer()  <span class="comment"># 替换成这样就好</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># 使用 Session</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    sess.run(init)</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">        sess.run(update)</span><br><span class="line">        print(sess.run(state))</span><br></pre></td></tr></table></figure><p>注意：直接 print(state) 不起作用！！</p><p>一定要把 sess 的指针指向 state 再进行 print 才能得到想要的结果！</p><h2 id="Placeholder-传入值"><a href="#Placeholder-传入值" class="headerlink" title="Placeholder 传入值"></a>Placeholder 传入值</h2><p>参考：</p><ul><li><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/2-5-placeholde/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/2-5-placeholde/</a></li></ul><p>placeholder 是 Tensorflow 中的占位符，暂时储存变量.</p><p>Tensorflow 如果想要从外部传入data, 那就需要用到 tf.placeholder(), 然后以这种形式传输数据 sess.run(<strong>*, feed_dict={input: </strong>}).</p><p>示例：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#在 Tensorflow 中需要定义 placeholder 的 type ，一般为 float32 形式</span></span><br><span class="line">input1 = tf.placeholder(tf.float32)</span><br><span class="line">input2 = tf.placeholder(tf.float32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># mul = multiply 是将input1和input2 做乘法运算，并输出为 output </span></span><br><span class="line">ouput = tf.multiply(input1, input2)</span><br></pre></td></tr></table></figure><p>接下来, 传值的工作交给了 sess.run() , 需要传入的值放在了feed_dict={} 并一一对应每一个 input. placeholder 与 feed_dict={} 是绑定在一起出现的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    print(sess.run(ouput, feed_dict=&#123;input1: [<span class="number">7.</span>], input2: [<span class="number">2.</span>]&#125;))</span><br><span class="line"><span class="comment"># [ 14.]</span></span><br></pre></td></tr></table></figure><h2 id="建造神经网络"><a href="#建造神经网络" class="headerlink" title="建造神经网络"></a>建造神经网络</h2><p>参考：</p><ul><li><a href="https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/3-2-create-NN/" target="_blank" rel="noopener">https://morvanzhou.github.io/tutorials/machine-learning/tensorflow/3-2-create-NN/</a></li></ul><h3 id="add-layer-功能"><a href="#add-layer-功能" class="headerlink" title="add_layer 功能"></a>add_layer 功能</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs, in_size, out_size, activation_function=None)</span>:</span></span><br><span class="line">    Weights = tf.Variable(tf.random_normal([in_size, out_size]))</span><br><span class="line">    biases = tf.Variable(tf.zeros([<span class="number">1</span>, out_size]) + <span class="number">0.1</span>)</span><br><span class="line">    Wx_plus_b = tf.matmul(inputs, Weights) + biases</span><br><span class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</span><br><span class="line">        outputs = Wx_plus_b</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        outputs = activation_function(Wx_plus_b)</span><br><span class="line">    <span class="keyword">return</span> outputs</span><br></pre></td></tr></table></figure><h3 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h3><p>构建所需的数据。 这里的x_data和y_data并不是严格的一元二次函数的关系，因为我们多加了一个noise,这样看起来会更像真实情况。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x_data = np.linspace(<span class="number">-1</span>,<span class="number">1</span>,<span class="number">300</span>, dtype=np.float32)[:, np.newaxis]</span><br><span class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">0.05</span>, x_data.shape).astype(np.float32)</span><br><span class="line">y_data = np.square(x_data) - <span class="number">0.5</span> + noise</span><br></pre></td></tr></table></figure><p>利用占位符定义我们所需的神经网络的输入。 tf.placeholder()就是代表占位符，这里的None代表无论输入有多少都可以，因为输入只有一个特征，所以这里是1。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xs = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span><br><span class="line">ys = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure><p>接下来，我们就可以开始定义神经层了。 通常神经层都包括输入层、隐藏层和输出层。这里的输入层只有一个属性， 所以我们就只有一个输入；隐藏层我们可以自己假设，这里我们假设隐藏层有10个神经元； 输出层和输入层的结构是一样的，所以我们的输出层也是只有一层。 所以，我们构建的是——输入层1个、隐藏层10个、输出层1个的神经网络。</p><h3 id="搭建网络"><a href="#搭建网络" class="headerlink" title="搭建网络"></a>搭建网络</h3><p>利用之前的add_layer()函数，这里使用 Tensorflow 自带的激励函数tf.nn.relu。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">l1 = add_layer(xs, <span class="number">1</span>, <span class="number">10</span>, activation_function=tf.nn.relu)</span><br></pre></td></tr></table></figure><p>接着，定义输出层。此时的输入就是隐藏层的输出——l1，输入有10层（隐藏层的输出层），输出有1层。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prediction = add_layer(l1, <span class="number">10</span>, <span class="number">1</span>, activation_function=<span class="keyword">None</span>)</span><br></pre></td></tr></table></figure><p>计算预测值prediction和真实值的误差，对二者差的平方求和再取平均。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss = tf.reduce_mean(tf.reduce_sum(tf.square(ys - prediction),</span><br><span class="line">                     reduction_indices=[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure><p>接下来，是很关键的一步，如何让机器学习提升它的准确率。tf.train.GradientDescentOptimizer()中的值通常都小于1，这里取的是0.1，代表以0.1的效率来最小化误差loss。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_step = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</span><br></pre></td></tr></table></figure><p>使用变量时，都要对它进行初始化，这是必不可少的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># init = tf.initialize_all_variables() # tf 马上就要废弃这种写法</span></span><br><span class="line">init = tf.global_variables_initializer()  <span class="comment"># 替换成这样就好</span></span><br></pre></td></tr></table></figure><p>定义Session，并用 Session 来执行 init 初始化步骤。 （注意：在tensorflow中，只有session.run()才会执行我们定义的运算。）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sess = tf.Session()</span><br><span class="line">sess.run(init)</span><br></pre></td></tr></table></figure><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>机器学习的内容是train_step, 用 Session 来 run 每一次 training 的数据，逐步提升神经网络的预测准确性。 (注意：当运算要用到placeholder时，就需要feed_dict这个字典来指定输入。)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># training</span></span><br><span class="line">    sess.run(train_step, feed_dict=&#123;xs: x_data, ys: y_data&#125;)</span><br></pre></td></tr></table></figure><p>每50步我们输出一下机器学习的误差。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> i % <span class="number">50</span> == <span class="number">0</span>:</span><br><span class="line">    <span class="comment"># to see the step improvement</span></span><br><span class="line">    print(sess.run(loss, feed_dict=&#123;xs: x_data, ys: y_data&#125;))</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">0.021204619</span></span><br><span class="line"><span class="number">0.009980676</span></span><br><span class="line"><span class="number">0.007174721</span></span><br><span class="line"><span class="number">0.006633012</span></span><br><span class="line"><span class="number">0.00622975</span></span><br><span class="line"><span class="number">0.005894037</span></span><br><span class="line"><span class="number">0.005621146</span></span><br><span class="line"><span class="number">0.0053801737</span></span><br><span class="line"><span class="number">0.00519997</span></span><br><span class="line"><span class="number">0.005050111</span></span><br><span class="line"><span class="number">0.004922069</span></span><br><span class="line"><span class="number">0.0048095705</span></span><br><span class="line"><span class="number">0.0047140927</span></span><br><span class="line"><span class="number">0.0046234317</span></span><br><span class="line"><span class="number">0.0045334958</span></span><br><span class="line"><span class="number">0.0044504963</span></span><br><span class="line"><span class="number">0.004378309</span></span><br><span class="line"><span class="number">0.0043256846</span></span><br><span class="line"><span class="number">0.0042802156</span></span><br><span class="line"><span class="number">0.0042369063</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
          <category> 深度学习 </category>
          
          <category> TF </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> TF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>实验室GPU服务器部署教程</title>
      <link href="/2018/03/29/%E5%AE%9E%E9%AA%8C%E5%AE%A4GPU%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%83%A8%E7%BD%B2%E6%95%99%E7%A8%8B/"/>
      <url>/2018/03/29/%E5%AE%9E%E9%AA%8C%E5%AE%A4GPU%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%83%A8%E7%BD%B2%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="Docker-安装"><a href="#Docker-安装" class="headerlink" title="Docker 安装"></a><a href="https://docs.docker.com/install/linux/ubuntu/" target="_blank" rel="noopener">Docker</a> 安装</h1><p>参考：<a href="https://docs.docker.com/install/linux/docker-ce/ubuntu/#set-up-the-repository" target="_blank" rel="noopener">https://docs.docker.com/install/linux/docker-ce/ubuntu/#set-up-the-repository</a></p><ol><li><p>Prerequisites</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">To install Docker CE, you need the 64-bit version of one of these Ubuntu versions:</span><br><span class="line"></span><br><span class="line">Artful 17.10 (Docker CE 17.11 Edge and higher only)</span><br><span class="line">Xenial 16.04 (LTS)</span><br><span class="line">Trusty 14.04 (LTS)</span><br></pre></td></tr></table></figure></li><li><p>Uninstall old versions</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get remove docker docker-engine docker.io</span><br></pre></td></tr></table></figure></li><li><p>Install Docker CE</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get update</span><br><span class="line"><span class="comment"># Install packages to allow apt to use a repository over HTTPS</span></span><br><span class="line">$ sudo apt-get install \</span><br><span class="line">    apt-transport-https \</span><br><span class="line">    ca-certificates \</span><br><span class="line">    curl \</span><br><span class="line">    software-properties-common</span><br><span class="line"><span class="comment"># Add Docker’s official GPG key:</span></span><br><span class="line">$ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -</span><br><span class="line">$ sudo apt-key fingerprint 0EBFCD88</span><br><span class="line"><span class="comment"># Use the following command to set up the stable repository.</span></span><br><span class="line">$ sudo add-apt-repository \</span><br><span class="line">   <span class="string">"deb [arch=amd64] https://download.docker.com/linux/ubuntu \</span></span><br><span class="line"><span class="string">   <span class="variable">$(lsb_release -cs)</span> \</span></span><br><span class="line"><span class="string">   stable"</span></span><br><span class="line">$ sudo apt-get update</span><br><span class="line"><span class="comment"># Install the latest version of Docker CE</span></span><br><span class="line">$ sudo apt-get install docker-ce</span><br><span class="line"><span class="comment"># Verify that Docker CE is installed correctly by running the hello-world image.</span></span><br><span class="line">$ sudo docker run hello-world</span><br></pre></td></tr></table></figure></li></ol><ol start="4"><li><p>UPGRADE DOCKER CE</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br></pre></td></tr></table></figure></li></ol><ol start="5"><li><p>Uninstall Docker CE</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo apt-get purge docker-ce</span><br><span class="line">$ sudo rm -rf /var/lib/docker</span><br></pre></td></tr></table></figure></li></ol><h1 id="Docker-备份、恢复和迁移"><a href="#Docker-备份、恢复和迁移" class="headerlink" title="Docker 备份、恢复和迁移"></a>Docker 备份、恢复和迁移</h1><h2 id="备份"><a href="#备份" class="headerlink" title="备份"></a>备份</h2><p>首先，为了备份Docker中的容器，我们会想看看我们想要备份的容器列表。要达成该目的，我们需要在我们运行着Docker引擎，并已创建了容器的Linux机器中运行 docker ps 命令。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># docker ps</span></span><br></pre></td></tr></table></figure><p>在此之后，我们要选择我们想要备份的容器，然后去创建该容器的快照。我们可以使用 docker commit 命令来创建快照。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># docker commit -p 30b8f18f20b4 container-backup</span></span><br></pre></td></tr></table></figure><p>该命令会生成一个作为Docker镜像的容器快照，我们可以通过运行 docker images 命令来查看Docker镜像，如下。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># docker images</span></span><br></pre></td></tr></table></figure><p>正如我们所看见的，上面做的快照已经作为Docker镜像保存了。现在，为了备份该快照，我们有两个选择，一个是我们可以登录进Docker注册中心，并推送该镜像；另一个是我们可以将Docker镜像打包成tar包备份，以供今后使用。</p><p>如果我们想要在<a href="https://hub.docker.com/" target="_blank" rel="noopener">Docker注册中心</a>上传或备份镜像，我们只需要运行 docker login 命令来登录进Docker注册中心，然后推送所需的镜像即可。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># docker login</span></span><br><span class="line"><span class="comment"># docker tag a25ddfec4d2a arunpyasi/container-backup:test</span></span><br><span class="line"><span class="comment"># docker push arunpyasi/container-backup</span></span><br></pre></td></tr></table></figure><p>如果我们不想备份到docker注册中心，而是想要将此镜像保存在本地机器中，以供日后使用，那么我们可以将其作为tar包备份。要完成该操作，我们需要运行以下 docker save 命令。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># docker save -o ~/container-backup.tar container-backup</span></span><br></pre></td></tr></table></figure><p>要验证tar包是否已经生成，我们只需要在保存tar包的目录中运行 ls 命令即可。</p><h2 id="恢复容器"><a href="#恢复容器" class="headerlink" title="恢复容器"></a>恢复容器</h2><p>接下来，在我们成功备份了我们的Docker容器后，我们现在来恢复这些制作了Docker镜像快照的容器。如果我们已经在注册中心推送了这些Docker镜像，那么我们仅仅需要把那个Docker镜像拖回并直接运行即可。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># docker pull arunpyasi/container-backup:test</span></span><br></pre></td></tr></table></figure><p>但是，如果我们将这些Docker镜像作为tar包文件备份到了本地，那么我们只要使用 docker load 命令，后面加上tar包的备份路径，就可以加载该Docker镜像了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># docker load -i ~/container-backup.tar</span></span><br></pre></td></tr></table></figure><p>现在，为了确保这些Docker镜像已经加载成功，我们来运行 docker images 命令。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># docker images</span></span><br></pre></td></tr></table></figure><p>在镜像被加载后，我们将用加载的镜像去运行Docker容器。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># docker run -d -p 80:80 container-backup</span></span><br></pre></td></tr></table></figure><h2 id="迁移Docker容器"><a href="#迁移Docker容器" class="headerlink" title="迁移Docker容器"></a>迁移Docker容器</h2><p>迁移容器同时涉及到了上面两个操作，备份和恢复。我们可以将任何一个Docker容器从一台机器迁移到另一台机器。在迁移过程中，首先我们将把容器备份为Docker镜像快照。然后，该Docker镜像或者是被推送到了Docker注册中心，或者被作为tar包文件保存到了本地。如果我们将镜像推送到了Docker注册中心，我们简单地从任何我们想要的机器上使用 docker run 命令来恢复并运行该容器。但是，如果我们将镜像打包成tar包备份到了本地，我们只需要拷贝或移动该镜像到我们想要的机器上，加载该镜像并运行需要的容器即可。</p><h1 id="Docker-SSH-访问"><a href="#Docker-SSH-访问" class="headerlink" title="Docker SSH 访问"></a>Docker SSH 访问</h1><p>假设我们已经pull了一个docker 镜像，如下图所示的tensorflow/tensorflow。</p><h2 id="启动容器"><a href="#启动容器" class="headerlink" title="启动容器"></a>启动容器</h2><pre><code><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">docker run --name my-tensorflow -it -p 8888:8888 -v ~/tensorflow:/<span class="built_in">test</span>/data tensorflow/tensorflow</span><br><span class="line"><span class="comment"># --name：创建的容器名，即my-tensorflow</span></span><br><span class="line"><span class="comment"># -it：保留命令行运行</span></span><br><span class="line"><span class="comment"># p 8888:8888：将本地的8888端口和http://localhost:8888/映射</span></span><br><span class="line"><span class="comment"># -v ~/tensorflow:/test/data:将本地的~/tensorflow挂载到容器内的/# test/data下</span></span><br><span class="line"><span class="comment"># tensorflow/tensorflow ：默认是tensorflow/tensorflow:latest,指定使用的镜像</span></span><br></pre></td></tr></table></figure></code></pre><p>如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --name tf tensorflow/tensorflow /bin/bash</span><br><span class="line"><span class="comment"># 这样我们就创建了名为tf的容器</span></span><br><span class="line">docker start tf</span><br><span class="line">docker attach tf</span><br></pre></td></tr></table></figure><h2 id="修改容器的root密码"><a href="#修改容器的root密码" class="headerlink" title="修改容器的root密码"></a>修改容器的root密码</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">apt-get install vim -y</span><br><span class="line">apt-get install openssh-server -y</span><br><span class="line">apt-get install passwd</span><br><span class="line">passwd root</span><br></pre></td></tr></table></figure><h2 id="修改ssh配置"><a href="#修改ssh配置" class="headerlink" title="修改ssh配置"></a>修改ssh配置</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/ssh/sshd_config</span><br><span class="line"><span class="comment"># 修改PermitRootLogin yes  </span></span><br><span class="line">UsePAM no</span><br></pre></td></tr></table></figure><h2 id="启动ssh服务"><a href="#启动ssh服务" class="headerlink" title="启动ssh服务"></a>启动ssh服务</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service ssh start</span><br></pre></td></tr></table></figure><h2 id="退出容器"><a href="#退出容器" class="headerlink" title="退出容器"></a>退出容器</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">exit</span></span><br></pre></td></tr></table></figure><h2 id="提交容器成为新的镜像"><a href="#提交容器成为新的镜像" class="headerlink" title="提交容器成为新的镜像"></a>提交容器成为新的镜像</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">例如叫做ubuntu-ssh，输入docker commit 容器ID ubuntu-ssh</span><br></pre></td></tr></table></figure><h2 id="启动这个镜像的容器，并映射本地的一个闲置的端口"><a href="#启动这个镜像的容器，并映射本地的一个闲置的端口" class="headerlink" title="启动这个镜像的容器，并映射本地的一个闲置的端口"></a>启动这个镜像的容器，并映射本地的一个闲置的端口</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it -p 50001:22 tf-ssh /bin/bash</span><br></pre></td></tr></table></figure><h2 id="ssh登录"><a href="#ssh登录" class="headerlink" title="ssh登录"></a>ssh登录</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh root@127.0.0.1 -p 50001</span><br></pre></td></tr></table></figure><h2 id="Docker后台运行"><a href="#Docker后台运行" class="headerlink" title="Docker后台运行"></a>Docker后台运行</h2><h1 id="阿里云加速器设置"><a href="#阿里云加速器设置" class="headerlink" title="阿里云加速器设置"></a>阿里云加速器设置</h1><p>由于官方Docker Hub网络速度较慢，这里使用阿里云提供的<a href="https://hub.docker.com/" target="_blank" rel="noopener">Docker Hub</a>. 需要配置阿里云加速器，官方说明如下：</p><ol><li><p>针对Docker客户端版本大于1.10的用户： </p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 您可以通过修改daemon配置文件/etc/docker/daemon.json来使用加速器</span></span><br><span class="line">$ sudo mkdir -p /etc/docker</span><br><span class="line">$ sudo tee /etc/docker/daemon.json &lt;&lt;-‘EOF’ </span><br><span class="line">  &#123; </span><br><span class="line">  “registry-mirrors”: [“https://fird1mfg.mirror.aliyuncs.com“] </span><br><span class="line">  &#125; </span><br><span class="line">  EOF</span><br><span class="line">$ sudo systemctl daemon-reload</span><br><span class="line">$ sudo systemctl restart docker</span><br></pre></td></tr></table></figure></li></ol><ol start="2"><li><p>针对Docker客户的版本小于等于1.10的用户或者想配置启动参数，可以使用下面的命令将配置添加到docker daemon的启动参数中.</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Ubuntu 12.04 14.04的用户:</span></span><br><span class="line">$ <span class="built_in">echo</span> “DOCKER_OPTS=/”<span class="variable">$DOCKER_OPTS</span> –registry-mirror=https://fird1mfg.mirror.aliyuncs.com/”” | sudo tee -a /etc/default/docker</span><br><span class="line">$ sudo service docker restart</span><br><span class="line"></span><br><span class="line"><span class="comment"># Ubuntu 15.04 16.04的用户</span></span><br><span class="line">$ sudo mkdir -p /etc/systemd/system/docker.service.d</span><br><span class="line">$ sudo tee /etc/systemd/system/docker.service.d/mirror.conf &lt;&lt;-‘EOF’ </span><br><span class="line">[Service] </span><br><span class="line">ExecStart=/usr/bin/docker daemon -H fd:// –registry-mirror=https://fird1mfg.mirror.aliyuncs.com </span><br><span class="line">EOF</span><br><span class="line">$ sudo systemctl daemon-reload</span><br><span class="line">$ sudo systemctl restart docker</span><br></pre></td></tr></table></figure></li></ol><h1 id="NVIDIA-Docker安装"><a href="#NVIDIA-Docker安装" class="headerlink" title="NVIDIA-Docker安装"></a><a href="https://github.com/NVIDIA/nvidia-docker/wiki" target="_blank" rel="noopener">NVIDIA-Docker</a>安装</h1><ol><li><p>Prerequisties</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">GNU/Linux x86_64 with kernel version &gt; 3.10 </span><br><span class="line">Docker &gt;= 1.9 (official docker-engine, docker-ce or docker-ee only) </span><br><span class="line">NVIDIA GPU with Architecture &gt; Fermi (2.1) </span><br><span class="line">NVIDIA drivers &gt;= 340.29 with binary nvidia-modprobe (驱动版本与CUDA计算能力相关)</span><br></pre></td></tr></table></figure></li><li><p>CUDA与NVIDIA driver安装 <a href="https://developer.nvidia.com/cuda-downloads" target="_blank" rel="noopener">cuda</a></p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">处理NVIDIA-Docker依赖项 NVIDIA drivers &gt;= 340.29 with binary nvidia-modprobe 要求. </span><br><span class="line">根据显卡，下载对应版本的CUDA并进行安装.</span><br></pre></td></tr></table></figure></li><li><p>NVIDIA-Docker安装</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Install nvidia-docker and nvidia-docker-plugin</span></span><br><span class="line"></span><br><span class="line">wget -P /tmp https://github.com/NVIDIA/nvidia-docker/releases/download/v1.0.1/nvidia-docker_1.0.1-1_amd64.deb</span><br><span class="line">sudo dpkg -i /tmp/nvidia-docker*.deb &amp;&amp; rm /tmp/nvidia-docker*.deb</span><br><span class="line"><span class="comment">#Test nvidia-smi</span></span><br><span class="line"></span><br><span class="line">sudo nvidia-docker run –rm nvidia/cuda nvidia-smi</span><br></pre></td></tr></table></figure></li></ol><ol start="4"><li><p>默认用nvdia-docker替代docker命令：</p> <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">'alias docker=nvidia-docker'</span> &gt;&gt; ~/.bashrc</span><br><span class="line">bash</span><br></pre></td></tr></table></figure></li></ol><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://jingyan.baidu.com/article/a3aad71aa180e7b1fa009676.html" target="_blank" rel="noopener">https://jingyan.baidu.com/article/a3aad71aa180e7b1fa009676.html</a></li><li><a href="https://github.com/ufoym/deepo#Installation" target="_blank" rel="noopener">https://github.com/ufoym/deepo#Installation</a></li><li><a href="https://hub.docker.com/r/ufoym/deepo/" target="_blank" rel="noopener">https://hub.docker.com/r/ufoym/deepo/</a></li><li><a href="https://github.com/fatedier/frp/blob/master/README_zh.md#frp-%E7%9A%84%E4%BD%9C%E7%94%A8" target="_blank" rel="noopener">https://github.com/fatedier/frp/blob/master/README_zh.md#frp-%E7%9A%84%E4%BD%9C%E7%94%A8</a></li><li><a href="https://ranpox.github.io/2018/01/14/notification-of-gpu-server/" target="_blank" rel="noopener">https://ranpox.github.io/2018/01/14/notification-of-gpu-server/</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> 编程 </category>
          
          <category> 深度学习 </category>
          
          <category> 环境配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
            <tag> 深度学习 </tag>
            
            <tag> 环境配置 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>categories</title>
      <link href="/2018/03/24/categories/"/>
      <url>/2018/03/24/categories/</url>
      
        <content type="html"><![CDATA[<h1 id="Categories"><a href="#Categories" class="headerlink" title="Categories"></a>Categories</h1><ul><li><p><font color="“blue”"><strong>编程: programming</strong></font></p><ul><li>C++</li><li>Java</li><li>Python</li><li>Matlab</li><li>Android</li><li>Linux</li><li>Git</li><li>Latex</li><li>PHP</li><li>IOS</li><li>数据库</li><li>网络编程</li><li>多线程</li><li>QT编程</li><li>MarkDown</li><li>DP<ul><li>Caffe</li><li>TF</li><li>PyTorch</li></ul></li><li>数据结构</li><li>算法</li></ul></li><li><p><font color="“blue”"><strong>学术: science</strong></font></p><ul><li>计算机视觉<ul><li>ICCV</li><li>CVPR</li><li>ECCV</li></ul></li><li>移动计算<ul><li>MobiCom</li><li>SigComm</li><li>InfoCom</li><li>NSDI</li><li>SenSys</li><li>PerCom</li><li>UbiComp</li></ul></li><li>机器学习&amp;深度学习<ul><li>AAAI</li><li>IJCAI</li></ul></li></ul></li><li><p><font color="“blue”"><strong>理论: theory</strong></font></p><ul><li>计算机视觉</li><li>移动计算</li><li>数学</li><li>优化算法</li><li>机器学习</li></ul></li><li><p><font color="“blue”"><strong>其他: other</strong></font></p><ul><li>Hexo博客</li><li>生活</li></ul></li></ul><h1 id="Tag"><a href="#Tag" class="headerlink" title="Tag"></a>Tag</h1><ul><li>论文</li><li>理论</li><li>其他</li><li>等等</li></ul>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
          <category> categories </category>
          
      </categories>
      
      
        <tags>
            
            <tag> categories </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
